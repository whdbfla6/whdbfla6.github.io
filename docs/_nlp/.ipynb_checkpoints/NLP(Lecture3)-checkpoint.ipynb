{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WcTtlntn5L-D"
   },
   "source": [
    "--- \n",
    "layout: single\n",
    "title: \n",
    "use_math: true\n",
    "comments: true\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nsIF2YE-6Gyr"
   },
   "source": [
    "본 글은 스탠포드 [CS 224N](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/) [Word Window Classification, Neural Networks, and Matrix Calculus] 강의를 들으며 정리한 글 입니다.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mlkv48tpK0nt"
   },
   "source": [
    "## 1. Classification\n",
    "NLP에서 분류는 단어, 문자, 문서 등을 x값으로 받아 감정, named entities 등 y값을 예측하는데 사용된다. binary case인 경우에는 시그모이드 함수를 사용하며, class가 3개 이상인 경우에는 **소프트맥스** 함수를 이용해 y값을 예측한다\n",
    "\n",
    "> Softmax\n",
    "\n",
    "소프트맥수 함수는 $pi =\\frac{e^{z_i}}{\\sum_{j=1}^{k}{e^{z_j}}}   $ 형태이며, 인풋값을 넣으면 0과 1 사이의 확률값으로 정규화해준다. class가 총 3개가 있다고 하자. 3차원 벡터 $z=[z_1,z_2,z_3]$의 입력을 받은 소프트맥수 함수의 출력값은 아래와 같다 \n",
    "\n",
    "![softmax](http://whdbfla6.github.io/assets/images/nlp3-1.JPG)\n",
    "\n",
    "$p_1,p_2,p_3$ 각각의 값은 1번 클래스가 정답일 확률, 2번 클래스가 정답일 확률, 3번 클래스가 정답일 확률을 나타내며 총 합은 1이다. \n",
    "\n",
    "![softmax](http://whdbfla6.github.io/assets/images/nlp3-2.JPG)\n",
    "\n",
    "실제 y값이 2번 클래스라고 하면 2번 클래스의 one-hot vector는 $[0,1,0]$다.이제 실제값과 예측값의 오차를 계산해 비용함수를 구해야 하는데, 일반적으로 Cross Entropy를 사용한다.\n",
    "\n",
    "> Cross Entropy \n",
    "\n",
    "Entropy는 불확실성의 척도다. 엔트로피가 높다는 것은 불확실성이 높다는 것을 의미하며, 수식은 아래의 형태와 같다.\n",
    "\n",
    "$H(p,q) = - \\sum_{c=1}^{C}{p(c)*log  q(c)}$ \n",
    "\n",
    "($p$는 실제 확률 분포, $q$는 계산된 확률)\n",
    "\n",
    "직관적으로 이해해보면 실제 y값은 $[0,1,0]$이고, 예측된 y값이 정확한 경우에 $q(c)$는 1이어야 한다. 이를 식에 대입해보면 $-1log(1) = 0$ 즉 Cross Entropy의 값이 0이 된다. 불확실성을 낮추기 위해서는 $- \\sum_{c=1}^{C}{p(c)*log  q(c)}$ 의 값을 최소화하는 방향으로 학습해야 한다. \n",
    "\n",
    "Cross Entropy loss function을 전체 데이터셋에 적용한 최종적인 비용 함수 형태는 다음과 같다.\n",
    "\n",
    "$J(theta) =  \\frac{1}{N}\\sum_{i=1}^{N}{-log(\\frac{e^{f_y}}{\\sum_{c=1}^{C}{e^{f_c}}})}$\n",
    "\n",
    "## 2. NN Classifiers\n",
    "\n",
    "![NN](http://whdbfla6.github.io/assets/images/nlp3-3.JPG)\n",
    "\n",
    "Neural network classifier는 nonlinear한 decision boundary를 구현하기 위해 등장했다. 앞서 배운 소프트맥스 함수의 decision boundary는 linear하기 때문에 매우 제한적이다. 왼쪽 그림을 살펴보면 빨간색 영역에 초록색 점들이 많은 것을 확인할 수 있는데, 이 문제를 해결하기 위해서는 직선이 아닌 곡선 형태의 boundary가 필요하다. 이 문제를 뉴럴 네트워크가 해결해 준다. \n",
    "\n",
    "![NN](http://whdbfla6.github.io/assets/images/nlp3-4.JPG)\n",
    "\n",
    "뉴럴 네트워크의 형태는 다음과 같다. input layer와 output layer의 사이에 은닉층이 존재하며, 이는 nonlinear한 decision boundary를 구현할 수 있도록 한다. 더 복잡한 문제를 해결하기 위해서는 수많은 은닉층을 추가하면 된다. 이 때 은닉층이 하나인 경우는 다층 퍼셉트론(MLP), 은닉층이 2개 이상인 경우 심층 신경망(DNN)이라 한다.\n",
    "\n",
    "![NN](http://whdbfla6.github.io/assets/images/nlp3-5.JPG)\n",
    "\n",
    "은닉층을 통과하고 나오면 최종 output을 도출하기 이전에 활성화 함수를 통과해야 하는데, 활성화 함수는 선형함수가 아닌 비선형 함수여야 한다. 선형함수를 사용하게 되면 hidden layer를 계속 쌓더라도 하나의 layer를 통과한 것과 차이가 없어진다. 예를들어 $f_1(x)=w_1x, f_2(x)=w_1x $라고 하면 $f_1(f_2(x))=w_1w_2x$로 선형함수 형태다. 즉 선형함수를 무수히 많이 통과하더라고 최종 결과는 하나의 선형 함수로 표현이 가능하며, 이는 신경망이 깊어지더라도 흥미로운 계산을 할 수 없음을 의미한다.  \n",
    "\n",
    "> NLP deep learning\n",
    "\n",
    "![NN](http://whdbfla6.github.io/assets/images/nlp3-6.JPG)\n",
    "\n",
    "NLP에서는 파라미터(w)와 word vector(x)를 같이 학습시킨다는 점에서 일반 딥러닝 학습과 차이가 있다. 이 때 훈련데이터가 매우 적은 상황에서 pre-trained word vector를 임베딩 벡터로 사용하기도 한다. 훈련데이터가 적은 경우에는 해당 문제에 특화된 임베딩 벡터를 만드는게 쉽지 않다고 한다. 따라서 해당 문제에 특화된 것은 아니지만 일반적이고 많은 훈련데이터로 이미 학습된 임베딩 벡터를 사용하면 성능을 개선할 수 있다.\n",
    "\n",
    "## 3. Window Classification\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vNyCf77j5pCb"
   },
   "source": [
    "참고문헌<br/>\n",
    "\n",
    "[딥러닝을 이용한 자연어 처리 입문](https://wikidocs.net/35476)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "NLP(Lecture3).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
