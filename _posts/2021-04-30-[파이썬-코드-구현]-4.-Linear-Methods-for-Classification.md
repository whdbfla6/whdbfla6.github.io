---
title: '[파이썬 코드 구현] 4. Linear Methods for Classification'
use_math: true
comments: true
layout: single
classes: wide
categories:

  - 머신러닝
  - ESL
  - 파이썬
---

## 1. LDA 코드 

코드

```python
import numpy as np
import pandas as pd
from numpy import transpose as t
from numpy import matmul
from numpy.linalg import inv
from numpy import linalg as LA
import matplotlib.pyplot as plt
import seaborn as sns
import random 
from sklearn.metrics import zero_one_loss
#from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
```


```python
class LinearDiscriminantAanlysis:
    
    def fit(self,x,y):
        self.unique,self.class_num= np.unique(y,return_counts=True)
        self.prior = []
        self.mu = []
        self.Sigma = 0
        for val in self.unique:
            self.prior.append(self.class_num[val]/len(y))
            ind = np.where(y==val)
            X = x[ind]
            self.mu.append(np.sum(X,axis=0)/len(X))
            self.Sigma += matmul(t(X-self.mu[val]),X-self.mu[val])
        self.Simga = np.array(self.Sigma)
        self.Sigma = self.Sigma/(len(x)-len(self.unique))
        
    def discriminant_function(self,x,prior,mu,Sigma):
        return matmul(x,matmul(inv(Sigma),t(mu)))-0.5*matmul(mu,matmul(inv(Sigma),t(mu)))+np.log(prior)
    
    def predict(self,x):
        c = []
        for i in x:
            result=[]
            for j in range(len(self.unique)):
                prior = self.prior[j]
                mu = self.mu[j]
                Sigma = self.Sigma
                result.append(self.discriminant_function(i,prior,mu,Sigma))
            c.append(result.index(max(result))) 
        return c
    
    def transformation(self,x):
        self.wc = self.Sigma*(len(x)-len(self.unique))
        total_mu = np.sum(x,axis=0)/len(x)
        self.bc = 0
        for i in range(len(self.unique)):
            n = self.class_num[i]
            self.bc += n*(np.outer((self.mu[i]-total_mu),(self.mu[i]-total_mu)))
        return self.wc, self.bc
    
    def fisherLDA(self,x,y,i):
        value,vector = LA.eig(matmul(inv(self.wc),self.bc))
        temp = pd.DataFrame(X)
        temp1 = pd.DataFrame(y)
        t = pd.concat([temp,temp1],axis=1)
        t.columns = ['x1','x2','y']
        plt.scatter(t['x1'],t['x2'],c=t['y'])
        x1 = np.linspace(min(X[:,0]),max(X[:,0]))
        for n,vec in enumerate(vector):
            if vector[n][0] ==0:
                plt.axvline(x=vector[0][0],label="eig"+str(n),c=np.random.rand(3,))
                plt.legend()
            else:
                inc = vector[n][1]/vector[n][0]
                plt.plot(x1,inc*(x1-vector[n][0])+vector[n][1],label="eig"+str(n),c=np.random.rand(3,))
                plt.legend()
        vec1 = np.array(vector[i]) #projection
        proj = matmul(np.outer(vec1,vec1),np.transpose(x))/(vec1.dot(vec1))
        plt.scatter(proj[0],proj[1])
        return proj
#[1,1] [vec[n][0],vec[n][1]] 
```

예시1.


```python
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
y = np.array([0,0,0,1,1,1])
```


```python
lda = LinearDiscriminantAanlysis()
```


```python
lda.fit(X,y)
```


```python
lda.transformation(X)
```


    (array([[4.        , 2.        ],
            [2.        , 1.33333333]]),
     array([[24.        , 16.        ],
            [16.        , 10.66666667]]))


```python
lda.fisherLDA(X,y,0)
```


    array([[ 0.,  0.,  0.,  0.,  0.,  0.],
           [-1., -1., -2.,  1.,  1.,  2.]])




![png](![4.1](http://whdbfla6.github.io/assets/package/output_7_1.png)



```python
lda.fisherLDA(X,y,1)
```




    array([[-0.09924301, -0.6901521 , -0.78939511,  0.09924301,  0.6901521 ,
             0.78939511],
           [ 0.08257517,  0.57424126,  0.65681643, -0.08257517, -0.57424126,
            -0.65681643]])



![png](http://whdbfla6.github.io/assets/package/output_8_1.png)




예시2.


```python
data = pd.read_csv("data/iris.csv")
```


```python
np.unique(data['Species'])
```


    array(['setosa', 'versicolor', 'virginica'], dtype=object)


```python
y = []
for i in data['Species']:
    if i == 'setosa':
        y.append(0)
    elif i == 'versicolor':
        y.append(1)
    else: y.append(2)
```


```python
X = data.iloc[:,1:5]
```


```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
X_train = np.array(X_train)
X_test = np.array(X_test)
```


```python
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
```


```python
lda = LinearDiscriminantAanlysis()
```


```python
lda.fit(X_train,y_train)
```


```python
y_pred = lda.predict(X_test)
```


```python
zero_one_loss(y_pred, y_test)
```


    0.0



## 2. Logistic Regression

코드

```python
from sklearn.datasets import load_iris
from sklearn.datasets import load_breast_cancer
from numpy.linalg import pinv,inv
from scipy.linalg import solve
import numpy as np
import pandas as pd
X, y = load_breast_cancer(return_X_y=True)
from numpy import transpose as t
from sklearn.metrics import zero_one_loss
```


```python
class LogisticRegression:
    def fit(self,x,y):
        n,p = x.shape
        beta = np.array([0]*p)
        max_iter = 0
        error=10000
        while error>0.1 and max_iter < 100:
            prob = np.exp(x.dot(beta))/(1+np.exp(x.dot(beta)))
            W = np.diag((1-np.array(prob))*np.array(prob))
            beta = beta+pinv(np.matmul(np.matmul(t(x),W),x)).dot(t(x)).dot(y-prob)
            max_iter += 1
            error = np.max(np.abs(y-prob))
        self.nbeta = beta
        print(max_iter)
    def pred(self,x):
        pred = []
        for i in x:
            prob = np.exp(i.dot(self.nbeta))/(1+np.exp(i.dot(self.nbeta)))
            if prob>0.5:
                pred.append(1)
            else:
                pred.append(0)
        return pred
```

예시1. 


```python
import pandas as pd
X, y = load_breast_cancer(return_X_y=True)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
X_train = np.array(X_train)
X_test = np.array(X_test)
```


```python
lg = LogisticRegression()
lg.fit(X_train,y_train)
```

    11

```python
pred = lg.pred(X_test)
```


```python
zero_one_loss(pred, y_test)
```


    0.05847953216374269

실제 패키지 비교


```python
from sklearn.linear_model import LogisticRegression
lg2 = LogisticRegression(max_iter=10000)
lg2.fit(X_train, y_train)
pred = lg2.predict(X_test)
zero_one_loss(pred,y_test)
```


    0.040935672514619936



## 3. Perceptron 알고리즘

```python
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
X, y = load_breast_cancer(return_X_y=True)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
```


```python
class perceptronAlgorithm(): 
    def __init__(self,max_iter=100,r=1):
        self.max_iter = max_iter
        self.r = r
    def fit(self,x,y):
        n,p = x.shape
        self.b0 = np.array([0])
        self.b = np.array([0]*p)
        y = [1 if dat==1 else -1 for dat in y]
        iteration = 0
        while iteration < self.max_iter:
            yc = [1 if self.b0+np.dot(dat,self.b)>0 else -1 for dat in x]
            for i in range(n):
                if y[i] != yc[i]:
                    self.b0 = self.b0 + self.r*y[i]
                    self.b = self.b + self.r*x[i]*y[i]
            iteration +=1
    def predict(self,x):
        pred = [1 if self.b0+np.dot(dat,self.b)>0 else 0 for dat in x]
        return pred
```

예시


```python
pc = perceptronAlgorithm(max_iter=10000,r=1)
```


```python
pc.fit(X_train,y_train)
```


```python
pred = pc.predict(X_test)
```


```python
from sklearn.metrics import zero_one_loss
zero_one_loss(pred, y_test)
```


    0.08771929824561409