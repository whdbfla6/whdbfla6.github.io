---
title: '[파이썬 코드 구현] 4. Linear Methods for Classification'
use_math: true
comments: true
layout: single
classes: wide
categories:

  - 머신러닝
  - ESL
  - 파이썬

tags:

  - 머신러닝
  - ESL
  - 파이썬
---



본 글은 ESL 4장을 기반으로 LDA, 로지스틱회귀, 퍼셉트론 알고리즘을 구현한 파이썬 코드를 담고 있습니다.
전체 코드는 [깃헙](https://github.com/whdbfla6/ESLpackage) 에서 확인하실 수 있습니다


## 1. LDA 코드 


```python
class LinearDiscriminantAanlysis:
    
    def fit(self,x,y):
        self.unique,self.class_num= np.unique(y,return_counts=True)
        self.prior = []
        self.mu = []
        self.Sigma = 0
        for val in self.unique:
            self.prior.append(self.class_num[val]/len(y))
            ind = np.where(y==val)
            X = x[ind]
            self.mu.append(np.sum(X,axis=0)/len(X))
            self.Sigma += matmul(t(X-self.mu[val]),X-self.mu[val])
        self.Simga = np.array(self.Sigma)
        self.Sigma = self.Sigma/(len(x)-len(self.unique))
        
    def discriminant_function(self,x,prior,mu,Sigma):
        return matmul(x,matmul(inv(Sigma),t(mu)))-0.5*matmul(mu,matmul(inv(Sigma),t(mu)))+np.log(prior)
    
    def predict(self,x):
        c = []
        for i in x:
            result=[]
            for j in range(len(self.unique)):
                prior = self.prior[j]
                mu = self.mu[j]
                Sigma = self.Sigma
                result.append(self.discriminant_function(i,prior,mu,Sigma))
            c.append(result.index(max(result))) 
        return c
    
    def transformation(self,x):
        self.wc = self.Sigma*(len(x)-len(self.unique))
        total_mu = np.sum(x,axis=0)/len(x)
        self.bc = 0
        for i in range(len(self.unique)):
            n = self.class_num[i]
            self.bc += n*(np.outer((self.mu[i]-total_mu),(self.mu[i]-total_mu)))
        return self.wc, self.bc
    
    def fisherLDA(self,x,y,i):
        value,vector = LA.eig(matmul(inv(self.wc),self.bc))
        temp = pd.DataFrame(X)
        temp1 = pd.DataFrame(y)
        t = pd.concat([temp,temp1],axis=1)
        t.columns = ['x1','x2','y']
        plt.scatter(t['x1'],t['x2'],c=t['y'])
        x1 = np.linspace(min(X[:,0]),max(X[:,0]))
        for n,vec in enumerate(vector):
            if vector[n][0] ==0:
                plt.axvline(x=vector[0][0],label="eig"+str(n),c=np.random.rand(3,))
                plt.legend()
            else:
                inc = vector[n][1]/vector[n][0]
                plt.plot(x1,inc*(x1-vector[n][0])+vector[n][1],label="eig"+str(n),c=np.random.rand(3,))
                plt.legend()
        vec1 = np.array(vector[i]) #projection
        proj = matmul(np.outer(vec1,vec1),np.transpose(x))/(vec1.dot(vec1))
        plt.scatter(proj[0],proj[1])
        return proj
#[1,1] [vec[n][0],vec[n][1]] 
```



## 2. Logistic Regression


```python
class LogisticRegression:
    def fit(self,x,y):
        n,p = x.shape
        beta = np.array([0]*p)
        max_iter = 0
        error=10000
        while error>0.1 and max_iter < 100:
            prob = np.exp(x.dot(beta))/(1+np.exp(x.dot(beta)))
            W = np.diag((1-np.array(prob))*np.array(prob))
            beta = beta+pinv(np.matmul(np.matmul(t(x),W),x)).dot(t(x)).dot(y-prob)
            max_iter += 1
            error = np.max(np.abs(y-prob))
        self.nbeta = beta
        print(max_iter)
    def pred(self,x):
        pred = []
        for i in x:
            prob = np.exp(i.dot(self.nbeta))/(1+np.exp(i.dot(self.nbeta)))
            if prob>0.5:
                pred.append(1)
            else:
                pred.append(0)
        return pred
```



## 3. Perceptron 알고리즘


```python
class perceptronAlgorithm(): 
    def __init__(self,max_iter=100,r=1):
        self.max_iter = max_iter
        self.r = r
    def fit(self,x,y):
        n,p = x.shape
        self.b0 = np.array([0])
        self.b = np.array([0]*p)
        y = [1 if dat==1 else -1 for dat in y]
        iteration = 0
        while iteration < self.max_iter:
            yc = [1 if self.b0+np.dot(dat,self.b)>0 else -1 for dat in x]
            for i in range(n):
                if y[i] != yc[i]:
                    self.b0 = self.b0 + self.r*y[i]
                    self.b = self.b + self.r*x[i]*y[i]
            iteration +=1
    def predict(self,x):
        pred = [1 if self.b0+np.dot(dat,self.b)>0 else 0 for dat in x]
        return pred
```
