---
title: '[ML] 5. Basis Expansions and Regularization'
use_math: true
comments: true
layout: single
classes: wide
categories:

  - 머신러닝
  - ESL

tags:
  
  - 머신러닝
  - ESL

---

## 1. Introduction

지금까지 다룬 linear regression, linear discriminant analysis, logistic regression은 모두 linearity를 가정하고 있다. linear model을 가정하는 경우 해석이 용이하고, 1차 Taylor 근사값이 된다. 하지만 linear additive한 모형으로는 설명할 수 없는 데이터들이 존재한다. 

이 단원에서는 $X$에 대한 linearity 가정을 하지 않고 $X$에 transformation을 하여 새롭게 생성된 공간에서 linear한 모형을 다룰 것이다. $h_m(X)$가 $X$에 대한 m번째 transformation이라고 할 때 모델은 다음과 같이 나타낼 수 있으며, $X$에 대한 **Linear basis expansion**이라고 부른다.


$$
f(X)=\sum_{m=1}^{M} \beta_{m} h_{m}(X)
$$


이 모델은 $X$ 자체에 대해서는 Non linear한 함수이지만 $h_1(X),\cdots,h_m(X)$ 에 대해서는 linear하다. $X$에 다양한 transformation이 이루어질 수 있는데 예는 다음과 같다.

- $h_m(X)=X_m$은 일반적인 linear model의 basis다
- $h_m(X)=X_j^2\ \text{or}\ h_m(X)=X_iX_j$는 높은 차수에 대한 Taylor expansion을 가능하게 하며, d차 다항식으로 표현이 가능하다
- $h_m(X)=log(X_j)$는 nonlinear한 transformation을 가능하게 한다

다음과 같이 $X$에 다양한 transformation을 취해 모델의 basis로 사용하게 되면 $f(x)$를 보다 flexible하게 나타낼 수 있다.

## 2. Piecewise Polynomials and Splines

이 단원에서 $X$는 one dimensional하다고 가정한다. 

**piecewise polynomial function**은 $X$의 정의역을 겹치지 않는 구간으로 나누어 각 구간 별로 다항식을 fitting하는 방식이다. 다음 그림은 piecewise constant의 예시로 3개의 basis를 사용하고 있다. 


$$
h_{1}(X)=I\left(X<\xi_{1}\right), \quad h_{2}(X)=I\left(\xi_{1} \leq X<\xi_{2}\right), \quad h_{3}(X)=I\left(\xi_{2} \leq X\right)
$$


정의역을 3개의 disjoint한 구간으로 나눈 것을 의미하며, 각 구간별로 least square estimate을 구하면 $\hat{\beta}_m=\bar{Y}_m$으로 각 구간에 포함된 데이터들의 평균이 beta값의 추정치가 된다.



<p align = "center"><img src="http://whdbfla6.github.io/assets/ml/5.1.PNG" alt="5" style="zoom:80%;"  /> </p>



> piecewise linear 

다음 그림은 **piecewise linear**의 예로 3개의 basis $h_{m+3}=h_{m}(X) X$ 가 추가된다. 하지만 일반적으로는 오른쪽과 같이 continuous한 function을 원하기 때문에 연속성 조건을 추가해야 한다. 각 knot에서의 연속성 조건이 추가되면 $6-2=4$개의 basis로 함수를 표현할 수 있다. 



<p align = "center"><img src="http://whdbfla6.github.io/assets/ml/5.2.PNG" alt="5" style="zoom:80%;"  /> </p>


$$
f\left(\xi_{1}^{-}\right)=f\left(\xi_{1}^{+}\right)\ \text {implies that }\ \beta_{1}+\xi_{1} \beta_{4}=\beta_{2}+\xi_{1} \beta_{5}\\ f\left(\xi_{2}^{-}\right)=f\left(\xi_{2}^{+}\right)\ \text {implies that }\ \beta_{2}+\xi_{2} \beta_{5}=\beta_{3}+\xi_{2} \beta_{6}
$$

$$
h_{1}(X)=1, \quad h_{2}(X)=X, \quad h_{3}(X)=\left(X-\xi_{1}\right)_{+}, \quad h_{4}(X)=\left(X-\xi_{2}\right)_{+}
$$



<p align = "center"><img src="http://whdbfla6.github.io/assets/ml/5.3.PNG" alt="5" style="zoom:80%;"  /> </p>



> piecewise cubic spline

**piecewise cubic spline은** 각 영역을 3차 다항식으로 fitting하며, 각 knot에서 연속성과 1차 2차 미분 값이 동일하다는 조건이 추가되어야 한다. 2차 미분값이 동일하다는 것은 기울기의 변화율이 동일한 것을 의미하며, 해당 조건을 추가해 더욱 smooth한 function을 얻을 수 있게 된다. 각 영역에서 4개의 파라미터가 필요하고, 각 knot에서 3개의 제약 term이 추가되기 때문에 $3(\text{region})\times 4- 2(\text{knot})\times 3 = 6$ 개의 basis로 구성된다. 


$$
\begin{array}{ll}
h_{1}(X)=1, & h_{3}(X)=X^{2}, & h_{5}(X)=\left(X-\xi_{1}\right)_{+}^{3} \\
h_{2}(X)=X, & h_{4}(X)=X^{3}, & h_{6}(X)=\left(X-\xi_{2}\right)_{+}^{3}
\end{array}
$$


일반화해서 **order-M spline**의 basis를 구하면 다음과 같으며, **truncated power basis**라고 불린다. 일반적으로 많이 쓰이는 order는 1,2,4이며 3차보다 더 높은 차수의 다항식은 잘 사용하지 않는다.


$$
\begin{aligned}
h_{j}(X) &=X^{j-1}, j=1, \ldots, M \\
h_{M+\ell}(X) &=\left(X-\xi_{\ell}\right)_{+}^{M-1}, \ell=1, \ldots, K
\end{aligned}
$$




### 2.1 Natural Cubic Splines

piecewise cubic spline은 양쪽 경계에서 분산이 커진다는 문제점이 있다. 해당 그림은 4개의 서로 다른 모델에 대한 Point wise variance를 나타낸 것인데, 0.33 0.66 point에 knot가 존재한다고 했을 때 <u>boundary에서 분산이 급격하게 커지는 것</u>을 확인할 수 있다. 정의역 양 끝은 연속성 제약이 없고 경계 바깥에 data가 존재하지 않기 때문에 분산이 커질 수 밖에 없다. 



<p align = "center"><img src="http://whdbfla6.github.io/assets/ml/5.4.PNG" alt="5" style="zoom:80%;"  /> </p>



natural cubic spline은 boundary knot 영역을 3차 다항식이 아닌 <u>1차 linear function으로 fittin</u>g해 variance 문제를 해결한다. 양 끝을 linear function으로 fitting한다는 제약이 추가되면서 $K$개의 knot를 사용할 경우 $K$개의 basis로 함수를 나타낼 수 있다.


$$
N_{1}(X)=1, \quad N_{2}(X)=X, \quad N_{k+2}(X)=d_{k}(X)-d_{K-1}(X)\\d_{k}(X)=\frac{\left(X-\xi_{k}\right)_{+}^{3}-\left(X-\xi_{K}\right)_{+}^{3}}{\xi_{K}-\xi_{k}}
$$


> 증명







## 4. Smoothing Splines

앞에서 살펴본 모델은 사전에 knot의 개수를 정해서 구간별로 regression을 fitting하는 방법론이었다. **smoothing spline**은 <u>knot를 데이터 수만큼 사용해서 knot를 선택할 필요가 없고, regularization term을 추가해 모형의 복잡도를</u> 결정할 수 있다. smoothing spline은 다음의 penalized residual sum of squares를 최소화하는 방식으로 함수가 결정되며, $\lambda$는 smoothing parameter로 복잡도를 결정해준다.
$$
\operatorname{RSS}(f, \lambda)=\sum_{i=1}^{N}\left\{y_{i}-f\left(x_{i}\right)\right\}^{2}+\lambda \int\left\{f^{\prime \prime}(t)\right\}^{2} d t
$$
$\lambda$가 0인 경우에는 데이터를 모두 지나는 어떤 함수든 RSS를 최소화하는 함수가 되며, $\lambda=\infty$ 인 경우 $f$의 이계도함수가 0이 되야하기 때문에 $f$는 일차함수가 된다. 

놀랍게도 위에서 정의한 RSS를 최소화하는 함수는 <u>각 데이터 point에서 knot를 갖는 natural cubic spline이 unique한 solution이 된다</u>. 따라서 함수 $f$는 N개의 basis 함수로 구성된 natural cubic spline이다.
$$
f(x)=\sum_{i=1}^{N} N_{j}(x) \theta_{j}
$$

> 증명



이 증명을 통해서 $f$를 추정하는 것은 $\hat\theta$를 추정하는 작업으로 간소화되어 RSS를 다음과 같이 나타낼 수 있다.
$$
\operatorname{RSS}(\theta, \lambda)=(\mathbf{y}-\mathbf{N} \theta)^{T}(\mathbf{y}-\mathbf{N} \theta)+\lambda \theta^{T} \boldsymbol{\Omega}_{N} \theta
$$


여기서 $N$행렬의 $(i,j)$ 성분은 $N_j(x_i)$이며 $\{ \boldsymbol{\Omega}_{N} \}_{j k}=\int  N_{j}^{\prime \prime}(t) N_{k}^{\prime \prime}(t) dt$ 이다

$\hat\theta$의 solution은 아래와 같으며 fitting된 smoothing spline은 다음과 같은 형태다.


$$
\hat{\theta}=\left(\mathbf{N}^{T} \mathbf{N}+\lambda \Omega_{N}\right)^{-1} \mathbf{N}^{T} \mathbf{y}\\ \hat{f}(x)=\sum_{j=1}^{N} N_{j}(x) \hat{\theta}_{j}
$$


### 4.1 Degrees of Freedom and Smoother Matrices

이 단원에서는 $\lambda$가 어떻게 결정되는지 살펴볼 것이다. theta의 추정치는 앞에서 살펴보았듯이 $y$에 linear한 형태다. $\hat{f}$은 아래와 같이 나타낼 수 있으며, $S_\lambda$는 **smoother matrix**로 불린다. 


$$
\begin{aligned}
\hat{\mathbf{f}} &=\mathbf{N}\left(\mathbf{N}^{T} \mathbf{N}+\lambda \boldsymbol{\Omega}_{N}\right)^{-1} \mathbf{N}^{T} \mathbf{y} \\
&=\mathbf{S}_{\lambda} \mathbf{y}
\end{aligned}
$$


fitting된 함수 또한 $y$에 linear한 form이며 $S_\lambda$는 $x_i$와 $\lambda$로만 구성된 행렬이다. 이제 smoother matrix를 일반적인 least square fitting과 비교해보자. $B_\epsilon$ 이 $N\times M$ 크기의 행렬로 $M$ cubic spline basis 라고 할 때 추정된 spline function은 다음과 같다.


$$
\begin{aligned}
\hat{\mathbf{f}} &=\mathbf{B}_{\xi}\left(\mathbf{B}_{\xi}^{T} \mathbf{B}_{\xi}\right)^{-1} \mathbf{B}_{\xi}^{T} \mathbf{y} \\
&=\mathbf{H}_{\xi} \mathbf{y} .
\end{aligned}
$$


$H_\epsilon$은 hat matrix라고 불리며 **projection matrix**에 해당한다. $H_\epsilon$와 $S_\lambda$의 특성을 살펴보면 다음과 같다.

- 두 행렬은 대칭이며, positive semi definite 행렬이다(고유값이 0 이상)

$$
X^\prime PX = X^\prime P^2X = (P^\prime X)^\prime (P^\prime X)>=0
$$

- $H^2=HH=H$로 Idempotent한 특성이 있으며, $S^2 = SS<=S$로 Shrinkage nature가 있다.
- $H_\epsilon$의 rank는 M $S_\lambda$의 rank는 N이다

$\Rightarrow$  $H_\epsilon y$는 y 벡터를 $B_\epsilon$ 의 M개로 구성된 column space에 정사영하는 것이기 때문에 $H_\epsilon$의 rank는 M이다
$$
\begin{aligned}
&\text { null }(N^T)=0 \quad \text { null }\left(N(N^T N+\lambda \Omega_ N)^{-1} N^{\top}\right)=0 \\
&\operatorname{rank}\left(N(N^T N+\lambda \Omega_N)^{-1} N^{\top}\right)=N-0=N
\end{aligned}
$$

- Projection matrix의 rank는 trace와 동일하다

$$

H_{\epsilon} x=\lambda x \quad H_\epsilon^{2} x=\lambda H_\epsilon x=\lambda^{2} x\\\ H_\epsilon^2 x=H_\epsilon x,\ \lambda^2 x = \lambda x\\
(x \neq 0) \quad \lambda(\lambda-1)=0\\
\rightarrow \text { projection matrix 의 고유값은 } 0 \text { 또는 } 1\\
\operatorname{tr}(H_\epsilon)=\operatorname{tr}\left(P \wedge P^{-1}\right)=\operatorname{tr}(\Lambda)=\lambda_{1}+\lambda_{2}+\cdots+\lambda_n
$$

독립인 column의 수만큼 고유값이 1의 값을 갖기 때문에 trace는 rank를 나타낸다. 

- smoothing spline의 effective degree of freedom(실제 자유도)은 $trace(S_\lambda)$ 이다. $S_\lambda$의 실제 rank는 $N$이지만 projection matrix와 다르게 각 basis에 대한 영향력을 $\lambda$를 통해서 줄이고 있다. 그 줄어든 영향력을 반영해서 실질적인 effective degree of freedom을 구하면 $trace(S_\lambda)$ 이다. $S_\lambda$의 고유값은 람다에 의해 0과 1사이의 값을 갖기 때문에 $trace(S_\lambda)$ 는 $N$보다 작은 값을 가질 것이고, 이는 실질적인 degree of freedom을 제공해 줄 것이다.


$$
\begin{aligned}
S_{\lambda}=N\left(N^{\top} N+\lambda \Omega_N\right)^{-1} N^{\top} &=\left(N^{-T}\left(N^{\top} N+\lambda \Omega_N\right) N^{-1}\right)^{-1} \\
&=\left(I+\lambda N^{-T} \Omega_N N^{-1}\right)^{-1} \\
&=(I+\lambda K)^{-1}
\end{aligned}
$$


$d_k$가 행렬 $K$에 대응되는 고유값이라고 할 때 $S_\lambda$의 고유값은 다음과 같다.


$$
p_k(\lambda)=\frac{1}{1+\lambda d_k}\\ \mathbf{S}_{\lambda}=\sum_{k=1}^{N} \frac{1}{1+\lambda d_{k}} \mathbf{u}_{k} \mathbf{u}_{k}^{T}
$$


람다값이 0인 경우 모든 고유값은 1이기 때문에 shrinkage 효과가 없으며 $\lambda$값을 조절해 각 고유값을 0과 1사이의 값으로 조정할 수 있다. 람다 값이 커지면 각 고유벡터에 대한 영향력이 줄어드는 것이기 때문에 복잡도가 줄어든 smooth한 spline을 얻을 수 있는 것이다.







