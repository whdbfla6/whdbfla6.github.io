---
title: '[ML] 5. Basis Expansions and Regularization'
use_math: true
comments: true
layout: single
classes: wide
categories:

  - 머신러닝
  - ESL

tags:
  
  - 머신러닝
  - ESL

---

## 1. Introduction

지금까지 다룬 linear regression, linear discriminant analysis, logistic regression은 모두 linearity를 가정하고 있다. linear model을 가정하는 경우 해석이 용이하고, 1차 Taylor 근사값이 된다. 하지만 linear additive한 모형으로는 설명할 수 없는 데이터들이 존재한다. 

이 단원에서는 $X$에 대한 linearity 가정을 하지 않고 $X$에 transformation을 하여 새롭게 생성된 공간에서 linear한 모형을 다룰 것이다. $h_m(X)$가 $X$에 대한 m번째 transformation이라고 할 때 모델은 다음과 같이 나타낼 수 있으며, $X$에 대한 **Linear basis expansion**이라고 부른다.


$$
f(X)=\sum_{m=1}^{M} \beta_{m} h_{m}(X)
$$


이 모델은 $X$ 자체에 대해서는 Non linear한 함수이지만 $h_1(X),\cdots,h_m(X)$ 에 대해서는 linear하다. $X$에 다양한 transformation이 이루어질 수 있는데 예는 다음과 같다.

- $h_m(X)=X_m$은 일반적인 linear model의 basis다
- $h_m(X)=X_j^2\ \text{or}\ h_m(X)=X_iX_j$는 높은 차수에 대한 Taylor expansion을 가능하게 하며, d차 다항식으로 표현이 가능하다
- $h_m(X)=log(X_j)$는 nonlinear한 transformation을 가능하게 한다

다음과 같이 $X$에 다양한 transformation을 취해 모델의 basis로 사용하게 되면 $f(x)$를 보다 flexible하게 나타낼 수 있다.

## 2. Piecewise Polynomials and Splines

이 단원에서 $X$는 one dimensional하다고 가정한다. 

**piecewise polynomial function**은 $X$의 정의역을 겹치지 않는 구간으로 나누어 각 구간 별로 다항식을 fitting하는 방식이다. 다음 그림은 piecewise constant의 예시로 3개의 basis를 사용하고 있다. 


$$
h_{1}(X)=I\left(X<\xi_{1}\right), \quad h_{2}(X)=I\left(\xi_{1} \leq X<\xi_{2}\right), \quad h_{3}(X)=I\left(\xi_{2} \leq X\right)
$$


정의역을 3개의 disjoint한 구간으로 나눈 것을 의미하며, 각 구간별로 least square estimate을 구하면 $\hat{\beta}_m=\bar{Y}_m$으로 각 구간에 포함된 데이터들의 평균이 beta값의 추정치가 된다.



<p align = "center"><img src="http://whdbfla6.github.io/assets/ml/5.1.PNG" alt="5" style="zoom:80%;"  /> </p>



다음 그림은 piecewise linear의 예로 3개의 basis $h_{m+3}=h_{m}(X) X$ 가 추가된다. 하지만 일반적으로는 오른쪽과 같이 continuous한 function을 원하기 때문에 연속성 조건을 추가해야 한다. 각 knot에서의 연속성 조건이 추가되면 $6-2=4$개의 basis로 함수를 표현할 수 있다. 



<p align = "center"><img src="http://whdbfla6.github.io/assets/ml/5.2.PNG" alt="5" style="zoom:80%;"  /> </p>


$$
f\left(\xi_{1}^{-}\right)=f\left(\xi_{1}^{+}\right)\ \text {implies that }\ \beta_{1}+\xi_{1} \beta_{4}=\beta_{2}+\xi_{1} \beta_{5}\\ f\left(\xi_{2}^{-}\right)=f\left(\xi_{2}^{+}\right)\ \text {implies that }\ \beta_{2}+\xi_{2} \beta_{5}=\beta_{3}+\xi_{2} \beta_{6}
$$

$$
h_{1}(X)=1, \quad h_{2}(X)=X, \quad h_{3}(X)=\left(X-\xi_{1}\right)_{+}, \quad h_{4}(X)=\left(X-\xi_{2}\right)_{+}
$$



<p align = "center"><img src="http://whdbfla6.github.io/assets/ml/5.3.PNG" alt="5" style="zoom:80%;"  /> </p>



**piecewise cubic spline은** 각 영역을 3차 다항식으로 fitting하며, 각 knot에서 연속성과 1차 2차 미분 값이 동일하다는 조건이 추가되어야 한다. 2차 미분값이 동일하다는 것은 기울기의 변화율이 동일한 것을 의미하며, 해당 조건을 추가해 더욱 smooth한 function을 얻을 수 있게 된다. 각 영역에서 4개의 파라미터가 필요하고, 각 knot에서 3개의 제약 term이 추가되기 때문에 $3(\text{region})\times 4- 2(\text{knot})\times 3 = 6$ 개의 basis로 구성된다. 


$$
\begin{array}{ll}
h_{1}(X)=1, & h_{3}(X)=X^{2}, & h_{5}(X)=\left(X-\xi_{1}\right)_{+}^{3} \\
h_{2}(X)=X, & h_{4}(X)=X^{3}, & h_{6}(X)=\left(X-\xi_{2}\right)_{+}^{3}
\end{array}
$$


일반화해서 **order-M spline**의 basis를 구하면 다음과 같으며, **truncated power basis**라고 불린다. 일반적으로 많이 쓰이는 order는 1,2,4이며 3차보다 더 높은 차수의 다항식은 잘 사용하지 않는다.


$$
\begin{aligned}
h_{j}(X) &=X^{j-1}, j=1, \ldots, M \\
h_{M+\ell}(X) &=\left(X-\xi_{\ell}\right)_{+}^{M-1}, \ell=1, \ldots, K
\end{aligned}
$$




### 2.1 Natural Cubic Splines



## 4. Smoothing Splines



### 4.1 Degrees of Freedom and Smoother Matrices

