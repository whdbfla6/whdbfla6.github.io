---
title: '[ML] 5. Basis Expansions and Regularization'
use_math: true
comments: true
layout: single
classes: wide
categories:

  - 머신러닝
  - ESL

tags:
  
  - 머신러닝
  - ESL

---

## 1. Introduction

지금까지 다룬 linear regression, linear discriminant analysis, logistic regression은 모두 linearity를 가정하고 있다. linear model을 가정하는 경우 해석이 용이하고, 1차 Taylor 근사값이 된다. 하지만 linear additive한 모형으로는 설명할 수 없는 데이터들이 존재한다. 

이 단원에서는 $X$에 대한 linearity 가정을 하지 않고 $X$에 transformation을 하여 새롭게 생성된 공간에서 linear한 모형을 다룰 것이다. $h_m(X)$가 $X$에 대한 m번째 transformation이라고 할 때 모델은 다음과 같이 나타낼 수 있으며, $X$에 대한 **Linear basis expansion**이라고 부른다.


$$
f(X)=\sum_{m=1}^{M} \beta_{m} h_{m}(X)
$$


이 모델은 $X$ 자체에 대해서는 Non linear한 함수이지만 $h_1(X),\cdots,h_m(X)$ 에 대해서는 linear하다. $X$에 다양한 transformation이 이루어질 수 있는데 예는 다음과 같다.

- $h_m(X)=X_m$은 일반적인 linear model의 basis다
- $h_m(X)=X_j^2\ \text{or}\ h_m(X)=X_iX_j$는 높은 차수에 대한 Taylor expansion을 가능하게 하며, d차 다항식으로 표현이 가능하다
- $h_m(X)=log(X_j)$는 nonlinear한 transformation을 가능하게 한다

다음과 같이 $X$에 다양한 transformation을 취해 모델의 basis로 사용하게 되면 $f(x)$를 보다 flexible하게 나타낼 수 있다.

## 2. Piecewise Polynomials and Splines

이 단원에서 $X$는 one dimensional하다고 가정한다. 

**piecewise polynomial function**은 $X$의 정의역을 겹치지 않는 구간으로 나누어 각 구간 별로 다항식을 fitting하는 방식이다. 다음 그림은 piecewise constant의 예시로 3개의 basis를 사용하고 있다. 


$$
h_{1}(X)=I\left(X<\xi_{1}\right), \quad h_{2}(X)=I\left(\xi_{1} \leq X<\xi_{2}\right), \quad h_{3}(X)=I\left(\xi_{2} \leq X\right)
$$


정의역을 3개의 disjoint한 구간으로 나눈 것을 의미하며, 각 구간별로 least square estimate을 구하면 $\hat{\beta}_m=\bar{Y}_m$으로 각 구간에 포함된 데이터들의 평균이 beta값의 추정치가 된다.



<p align = "center"><img src="http://whdbfla6.github.io/assets/ml/5.1.PNG" alt="5" style="zoom:80%;"  /> </p>



> piecewise linear 

다음 그림은 **piecewise linear**의 예로 3개의 basis $h_{m+3}=h_{m}(X) X$ 가 추가된다. 하지만 일반적으로는 오른쪽과 같이 continuous한 function을 원하기 때문에 연속성 조건을 추가해야 한다. 각 knot에서의 연속성 조건이 추가되면 $6-2=4$개의 basis로 함수를 표현할 수 있다. 



<p align = "center"><img src="http://whdbfla6.github.io/assets/ml/5.2.PNG" alt="5" style="zoom:80%;"  /> </p>


$$
f\left(\xi_{1}^{-}\right)=f\left(\xi_{1}^{+}\right)\ \text {implies that }\ \beta_{1}+\xi_{1} \beta_{4}=\beta_{2}+\xi_{1} \beta_{5}\\ f\left(\xi_{2}^{-}\right)=f\left(\xi_{2}^{+}\right)\ \text {implies that }\ \beta_{2}+\xi_{2} \beta_{5}=\beta_{3}+\xi_{2} \beta_{6}
$$

$$
h_{1}(X)=1, \quad h_{2}(X)=X, \quad h_{3}(X)=\left(X-\xi_{1}\right)_{+}, \quad h_{4}(X)=\left(X-\xi_{2}\right)_{+}
$$



<p align = "center"><img src="http://whdbfla6.github.io/assets/ml/5.3.PNG" alt="5" style="zoom:80%;"  /> </p>



> piecewise cubic spline

**piecewise cubic spline은** 각 영역을 3차 다항식으로 fitting하며, 각 knot에서 연속성과 1차 2차 미분 값이 동일하다는 조건이 추가되어야 한다. 2차 미분값이 동일하다는 것은 기울기의 변화율이 동일한 것을 의미하며, 해당 조건을 추가해 더욱 smooth한 function을 얻을 수 있게 된다. 각 영역에서 4개의 파라미터가 필요하고, 각 knot에서 3개의 제약 term이 추가되기 때문에 $3(\text{region})\times 4- 2(\text{knot})\times 3 = 6$ 개의 basis로 구성된다. 


$$
\begin{array}{ll}
h_{1}(X)=1, & h_{3}(X)=X^{2}, & h_{5}(X)=\left(X-\xi_{1}\right)_{+}^{3} \\
h_{2}(X)=X, & h_{4}(X)=X^{3}, & h_{6}(X)=\left(X-\xi_{2}\right)_{+}^{3}
\end{array}
$$


일반화해서 **order-M spline**의 basis를 구하면 다음과 같으며, **truncated power basis**라고 불린다. 일반적으로 많이 쓰이는 order는 1,2,4이며 3차보다 더 높은 차수의 다항식은 잘 사용하지 않는다.


$$
\begin{aligned}
h_{j}(X) &=X^{j-1}, j=1, \ldots, M \\
h_{M+\ell}(X) &=\left(X-\xi_{\ell}\right)_{+}^{M-1}, \ell=1, \ldots, K
\end{aligned}
$$




### 2.1 Natural Cubic Splines

piecewise cubic spline은 양쪽 경계에서 분산이 커진다는 문제점이 있다. 해당 그림은 4개의 서로 다른 모델에 대한 Pointwise variance를 나타낸 것인데, 0.33 0.66 point에 knot가 존재하며 boundary에서 분산이 급격하게 커지는 것을 확인할 수 있다. 정의역 양 끝은 연속성 제약이 없고 경계 바깥에 data가 존재하지 않기 때문에 분산이 커질 수 밖에 없다. 



<p align = "center"><img src="http://whdbfla6.github.io/assets/ml/5.4.PNG" alt="5" style="zoom:80%;"  /> </p>



natural cubic spline은 boundary knot 영역을 3차 다항식이 아닌 1차 linear function으로 fitting해 variance 문제를 해결한다. 양 끝을 linear function으로 fitting한다는 제약이 추가되면서 $K$개의 knot를 사용할 경우 $K$개의 basis로 함수를 나타낼 수 있다.


$$
N_{1}(X)=1, \quad N_{2}(X)=X, \quad N_{k+2}(X)=d_{k}(X)-d_{K-1}(X)\\d_{k}(X)=\frac{\left(X-\xi_{k}\right)_{+}^{3}-\left(X-\xi_{K}\right)_{+}^{3}}{\xi_{K}-\xi_{k}}
$$


> 증명







## 4. Smoothing Splines

앞에서 살펴본 모델은 사전에 knot의 개수를 정해서 구간별로 regression을 fitting하는 방법론이었다. **smoothing spline**은 knot를 데이터 수만큼 사용해서 knot를 선택할 필요가 없고, regularization term을 추가해 모형의 복잡도를 결정할 수 있다. smoothing spline은 다음의 penalized residual sum of squares를 최소화하는 방식으로 함수가 결정되며, $\lambda$는 smoothing parameter로 복잡도를 결정해준다.
$$
\operatorname{RSS}(f, \lambda)=\sum_{i=1}^{N}\left\{y_{i}-f\left(x_{i}\right)\right\}^{2}+\lambda \int\left\{f^{\prime \prime}(t)\right\}^{2} d t
$$
$\lambda$가 0인 경우에는 데이터를 모두 지나는 어떤 함수든 RSS를 최소화하는 함수가 되며, $\lambda=\infin$ 인 경우 $f$의 이계도함수가 0이 되야하기 때문에 $f$는 일차함수가 된다. 

놀랍게도 위에서 정의한 RSS를 최소화하는 함수는 <u>각 데이터 point에서 knot를 갖는 natural cubic spline이 unique한 solution이 된다</u>. 따라서 함수 $f$는 N개의 basis 함수로 구성된 natural cubic spline이다.
$$
f(x)=\sum_{i=1}^{N} N_{j}(x) \theta_{j}
$$

> 증명



이 증명을 통해서 $f$를 추정하는 것은 $\hat\theta$를 추정하는 작업으로 간소화되어 RSS를 다음과 같이 나타낼 수 있다.
$$
\operatorname{RSS}(\theta, \lambda)=(\mathbf{y}-\mathbf{N} \theta)^{T}(\mathbf{y}-\mathbf{N} \theta)+\lambda \theta^{T} \boldsymbol{\Omega}_{N} \theta
$$


여기서 $N$행렬의 $(i,j)$ 성분은 $N_j(x_i)$이며 $\left\{\boldsymbol{\Omega}_{N}\right\}_{j k}=\int N_{j}^{\prime \prime}(t) N_{k}^{\prime \prime}(t) d t$ 이다

$\hat\theta$의 solution은 아래와 같으며 fitting된 smoothing spline은 다음과 같은 형태다.
$$
\hat{\theta}=\left(\mathbf{N}^{T} \mathbf{N}+\lambda \Omega_{N}\right)^{-1} \mathbf{N}^{T} \mathbf{y}\\ \hat{f}(x)=\sum_{j=1}^{N} N_{j}(x) \hat{\theta}_{j}
$$


### 4.1 Degrees of Freedom and Smoother Matrices

