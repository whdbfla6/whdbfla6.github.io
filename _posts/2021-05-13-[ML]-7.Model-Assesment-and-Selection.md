---
title: '[ML] 7.Model Assesment and Selection'
use_math: true
comments: true
layout: single
classes: wide
categories:

  - ë¨¸ì‹ ëŸ¬ë‹
  - ESL

tags:
  
  - ë¨¸ì‹ ëŸ¬ë‹
  - ESL
---





ì´ ë‹¨ì›ì—ì„œëŠ” ëª¨í˜• ì„±ëŠ¥ í‰ê°€ì™€, í‰ê°€ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª¨í˜•ì„ ì„ íƒí•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ë‹¤ë£° ê²ƒì´ë‹¤. 

## 2. Bias, Variance and Model Complexity

input vector $X$ target variable $Y$ ê·¸ë¦¬ê³  train dataë¡œë¶€í„° ì¶”ì •ëœ $\hat{f}(X)$ ì˜ˆì¸¡ëª¨ë¸ì´ ìˆë‹¤ê³  í•˜ì. **Loss function**ì€ ì‹¤ì œ $Y$ê°’ê³¼ ëª¨ë¸ë¡œë¶€í„° ì˜ˆì¸¡í•œ $\hat{f}(X)$ ì‚¬ì´ì˜ errorë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. Loss function $L(Y,\hat{f}(X))$ ì„ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì€ ì—¬ëŸ¬ê°€ì§€ê°€ ìˆëŠ”ë° ì¼ë°˜ì ìœ¼ë¡œ ì ˆëŒ€ê°’ì˜ ì°¨ì´ë‚˜ ì°¨ì´ì˜ ì œê³±ì„ ì´ìš©í•œë‹¤. 


$$
L(Y, \hat{f}(X))=\left\{\begin{array}{ll}
(Y-\hat{f}(X))^{2} & \text { squared error } \\
|Y-\hat{f}(X)| & \text { absolute error }
\end{array}\right.
$$


ì—¬ê¸°ì„œ **Test error(Generalization error)**ëŠ” ë…ë¦½ì ì¸ test sampleì— ëŒ€í•œ prediction errorë‹¤.  train sample $\mathcal{T}$ ì€ ì£¼ì–´ì§„ ìƒíƒœì´ë©°, test errorëŠ” íŠ¹ì •í•œ training setì— ëŒ€í•œ errorë¥¼ ë‚˜íƒ€ë‚¸ë‹¤


$$
\operatorname{Err}_{\mathcal{T}}=\mathrm{E}[L(Y, \hat{f}(X)) \mid \mathcal{T}]
$$


ì´ë²ˆì—ëŠ” train setì´ ê³ ì •ëœ ê²ƒì´ ì•„ë‹ˆë¼ randomí•˜ê²Œ ì£¼ì–´ì§„ ìƒí™©ì—ì„œ ê¸°ëŒ€ê°’ì„ êµ¬í•´ë³´ì. ì´ë¥¼ **expected test error** í˜¹ì€ **expected prediction error**ë¼ í•˜ë©° ë‹¤ìŒê³¼ ê°™ì´ êµ¬í•  ìˆ˜ ìˆë‹¤. 


$$
\operatorname{Err}=\mathrm{E}[L(Y, \hat{f}(X))]=\mathrm{E}\left[\operatorname{Err}_{\mathcal{T}}\right]
$$



![7](http://whdbfla6.github.io/assets/ml/7.1.PNG)



ì´ ê·¸ë¦¼ì—ì„œ ì—°í•œ red ì»¤ë¸ŒëŠ” 100ê°œ train set ê°ê°ì— ëŒ€í•œ $Err_{\mathcal{T}}$ ì„ ë‚˜íƒ€ë‚¸ë‹¤. ì§„í•œ red ì»¤ë¸ŒëŠ” 100ê°œì˜ $Err_{\mathcal{T}}$ ì— ëŒ€í•œ ê¸°ëŒ€ê°’ìœ¼ë¡œ expected prediction errorë‹¤. ìš°ë¦¬ì˜ ëª©í‘œëŠ” train setì´ ì£¼ì–´ì¡Œì„ ë•Œ errorì— ëŒ€í•œ ê¸°ëŒ€ê°’ì„ êµ¬í•˜ëŠ” ê²ƒì´ì§€ë§Œ ëŒ€ë¶€ë¶„ì˜ ë°©ë²•ë¡ ì€ errorì˜ ê¸°ëŒ€ê°’ì„ ì¶”ì •í•˜ê³  ìˆë‹¤. 

ëª¨ë¸ì˜ êµ¬ì¡°ê°€ ë³µì¡í•´ì§€ëŠ” ê²½ìš° biasëŠ” ê°ì†Œí•˜ì§€ë§Œ varianceê°€ ì»¤ì§€ëŠ” ë¬¸ì œê°€ ìƒê¸´ë‹¤. ì¦‰ ìƒˆë¡œìš´ ë°ì´í„°ê°€ ë“¤ì–´ì™”ì„ ë•Œ ì˜ˆì¸¡ì„ ì˜ ëª»í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. biasì™€ varianceë¥¼ ëª¨ë‘ ì¤„ì—¬ expected test errorë¥¼ ìµœì†Œí™”í•˜ëŠ” ëª¨ë¸ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.

**training error**ëŠ” ëª¨ë“  train sampleì— ëŒ€í•´ lossê°’ì„ ê³„ì‚°í•´ í‰ê· ì„ êµ¬í•œ ê²ƒì´ë‹¤

$$
\overline{\mathrm{err}}=\frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, \hat{f}\left(x_{i}\right)\right) .
$$


train errorì˜ ê²½ìš° ëª¨ë¸ì˜ êµ¬ì¡°ê°€ ë³µì¡í•´ì§ˆìˆ˜ë¡ ì‘ì€ ê°’ì„ ê°€ì ¸ ëª¨ë¸ì˜ ë³µì¡ë„ê°€ ì¶©ë¶„íˆ í¬ë‹¤ë©´(ex.ë°ì´í„°ë¥¼ ëª¨ë‘ ì—°ê²°í•œ function) 0ì˜ ê°’ì„ ê°–ëŠ”ë‹¤. ë”°ë¼ì„œ training errorëŠ” test errorì— ëŒ€í•œ ì¶”ì •ì¹˜ë¡œ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤. ì¼ë°˜ì ìœ¼ë¡œëŠ” ë°ì´í„°ë¥¼ ```training set, validation set, test set```ìœ¼ë¡œ ë¶„ë¦¬í•´ training setì€ ëª¨ë¸ì„ fittingí•˜ëŠ” ê²½ìš° validation setì€ ëª¨ë¸ ì„ íƒì„ ìœ„í•´ prediction errorë¥¼ ì¶”ì •í•˜ëŠ” ê²½ìš°, test setì€ ìµœì¢…ì ìœ¼ë¡œ ì„ íƒí•œ ëª¨ë¸ì— ëŒ€í•œ prediction errorë¥¼ êµ¬í•˜ëŠ”ë° ì‚¬ìš©ëœë‹¤. ì—¬ê¸°ì„œ ëª¨í˜• ì„ íƒì€ ì„œë¡œ ë‹¤ë¥¸ ëª¨í˜•ì˜ ì„±ëŠ¥ì„ ì¶”ì •í•´ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ê°€ì§„ ëª¨í˜•ì„ ê³ ë¥´ëŠ” ê³¼ì •ì´ë©°, ëª¨í˜• í‰ê°€ëŠ” ëª¨í˜•ì„ ì„ íƒí•œ í›„ì— ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•œ Generalization errorë¥¼ ì¶”ì •í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.  ì¼ë°˜ì ìœ¼ë¡œ ì „ì²´ ë°ì´í„° ì…‹ì˜ ë°˜ì„ training set ë‚˜ë¨¸ì§€ì˜ 1/2ì„ ê°ê° validation setê³¼ test setìœ¼ë¡œ ì‚¬ìš©í•œë‹¤. í•˜ì§€ë§Œ ëŒ€ë¶€ë¶„ì˜ ìƒí™©ì—ì„œëŠ” ë°ì´í„°ì˜ ì–‘ì´ ì¶©ë¶„í•˜ì§€ ì•Šì•„ ì„¸ ê°œì˜ íŒŒíŠ¸ë¡œ ë‚˜ëˆŒ ìˆ˜ê°€ ì—†ë‹¤. ë”°ë¼ì„œ ì´í›„ì˜ ë‹¨ì›ì—ì„œëŠ” ì´ëŸ¬í•œ ìƒí™©ì—ì„œ ëª¨ë¸ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ë‹¤ë£° ê²ƒì´ë‹¤. 



## 3. The Bias-Variance Decomposition

$Y=f(X)+\epsilon\quad \text{where}\quad E(\epsilon)=0\ Var(\epsilon)=\sigma^2$  ëª¨í˜•ì„ ê°€ì •í•œë‹¤ê³  í•˜ì. íŠ¹ì • í¬ì¸íŠ¸  $X=x_0$ ì— ëŒ€í•œ íšŒê·€ì‹ $\hat{f}(X)$ì˜ expected prediction errorë¥¼ êµ¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.


$$
\begin{aligned}
\operatorname{Err}\left(x_{0}\right) &=E\left[\left(Y-\hat{f}\left(x_{0}\right)\right)^{2} \mid X=x_{0}\right] \\
&=\sigma_{\varepsilon}^{2}+\left[\mathrm{E} \hat{f}\left(x_{0}\right)-f\left(x_{0}\right)\right]^{2}+E\left[\hat{f}\left(x_{0}\right)-\mathrm{E} \hat{f}\left(x_{0}\right)\right]^{2} \\
&=\sigma_{\varepsilon}^{2}+\operatorname{Bias}^{2}\left(\hat{f}\left(x_{0}\right)\right)+\operatorname{Var}\left(\hat{f}\left(x_{0}\right)\right)
\end{aligned}
$$


ì²«ë²ˆì§¸ termì€ ì–¼ë§ˆë‚˜ $f(X_0)$ë¥¼ ì˜ ì˜ˆì¸¡í–ˆë“ ì§€ì— ìƒê´€ì—†ì´ ì¤„ì¼ ìˆ˜ ì—†ëŠ” errorë‹¤. ë‘ë²ˆì§¸ termì€ biasë¥¼ ì œê³±í•œ ë¶€ë¶„ìœ¼ë¡œ ì¶”ì •ì¹˜ì˜ í‰ê· ê°’ê³¼ true meanì˜ ì°¨ì´ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ë§ˆì§€ë§‰ ë¶€ë¶„ì€ varianceë¡œ $\hat{f}(x_0)$ê°€ ê·¸ í‰ê· ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ì–¼ë§ˆë§Œí¼ì˜ í­ìœ¼ë¡œ ë³€ë™í•˜ëŠ” ì§€ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ì•ì„œ ì„¤ëª…í–ˆë“¯ì´ ëª¨í˜•ì˜ ë³µì¡ë„ê°€ ì»¤ì§ˆìˆ˜ë¡ biasëŠ” ì¤„ì–´ë“¤ì§€ë§Œ varianceê°€ ì»¤ì§„ë‹¤.



> EX1 . KNN

K-nearest-neighbor regression fitì˜ ê²½ìš°ì— squared error lossëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. ì‹ì—ì„œ kê°€ ì»¤ì§€ëŠ” ê²½ìš° biasëŠ” ì»¤ì§€ê³  ë¶„ì‚°ì´ ì»¤ì§€ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.


$$
\begin{aligned}
\operatorname{Err}\left(x_{0}\right) &=E\left[\left(Y-\hat{f}_{k}\left(x_{0}\right)\right)^{2} \mid X=x_{0}\right] \\
&=\sigma_{\varepsilon}^{2}+\left[f\left(x_{0}\right)-\frac{1}{k} \sum_{\ell=1}^{k} f\left(x_{(\ell)}\right)\right]^{2} + \frac{1}{k^2}k\sigma^2_{\epsilon} \\&=\sigma_{\varepsilon}^{2}+\left[f\left(x_{0}\right)-\frac{1}{k} \sum_{\ell=1}^{k} f\left(x_{(\ell)}\right)\right]^{2} + \frac{1}{k}\sigma^2_{\epsilon}
\end{aligned}
$$


> EX2. Linear Model

pê°œì˜ componentsë¡œ êµ¬ì„±ëœ íŒŒë¼ë¯¸í„° ë²¡í„°ë¥¼ $\beta$ë¼ê³  í•  ë•Œ linear model $\hat{f}_p(x)=x^T\beta$ì˜ squared error lossëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.


$$
\begin{aligned}
\operatorname{Err}\left(x_{0}\right) &=E\left[\left(Y-\hat{f}_{k}\left(x_{0}\right)\right)^{2} \mid X=x_{0}\right] \\ &=\sigma_{\varepsilon}^{2}+\left[f\left(x_{0}\right)-\mathrm{E} \hat{f}_{p}\left(x_{0}\right)\right]^{2}+\left\|\mathbf{h}\left(x_{0}\right)\right\|^{2} \sigma_{\varepsilon}^{2}
\end{aligned}\\
\text{where}\quad \mathbf{h}\left(x_{0}\right)=\mathbf{X}\left(\mathbf{X}^{T} \mathbf{X}\right)^{-1} x_{0}
$$


íšŒê·€ ì¶”ì •ì‹ ì— ëŒ€í•´ ë¶„ì‚°ê°’ì„ êµ¬í•˜ë©´  $x_0^T(X^TX)^{-1}X^TX(X^TX)^{-1}x_0\sigma_{\epsilon}^2$ ë¡œ ì •ë¦¬í•˜ë©´, $x_0^T(X^TX)^{-1}x_0\sigma_{\epsilon}^2$ì´ë‹¤. ë¶„ì‚°ì€ ê° í¬ì¸íŠ¸ $x_0$ì— ë”°ë¼ ë‹¤ë¥¸ ê°’ì„ ê°€ì§€ë©° í‰ê· ê°’ì„ êµ¬í•˜ë©´ $\frac{p}{N}\sigma_{\epsilon}^2$ ì´ë‹¤. 


$$
\begin{aligned}
\frac{1}{N}\sum_{i=1}^Nx_i^T(X^TX)^{-1}x_i\sigma_{\epsilon}^2&=\sigma_{\epsilon}^2\frac{1}{N}\sum_{i=1}^Nx_i^T(X^TX)^{-1}x_i\\ &= \frac{1}{N}tr(X(X^TX)^{-1}X^T)\sigma_{\epsilon}^2\\ &= \frac{1}{N}tr(I_p)\sigma_{\epsilon}^2 \\ &= \frac{p}{N}\sigma_{\epsilon}^2
\end{aligned}
$$

> ridge regression bias


ridge regressionì˜ ê²½ìš° ëª¨ë¸ì˜ biasë¥¼ best fitting linear modelì—ì„œ ë°œìƒí•˜ëŠ” biasì™€ ë‚˜ë¨¸ì§€ ë¶€ë¶„ì˜ í•©ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.  

($E(f(X)-X^T\beta)^2$ ì„ ìµœì†Œí™” í•˜ëŠ” $\beta$ë¥¼  $\beta_{\ast}$ë¼ê³  í•  ë•Œ $\beta_{\ast}$ ëŠ” best fitting íšŒê·€ì‹ì˜ ë² íƒ€ ì¶”ì •ì¹˜ë‹¤)


$$
\begin{aligned}
\mathrm{E}_{x_{0}}\left[f\left(x_{0}\right)-\mathrm{E} \hat{f}_{\alpha}\left(x_{0}\right)\right]^{2} &=\mathrm{E}_{x_{0}}\left[f\left(x_{0}\right)-x_{0}^{T} \beta_{*} + x_{0}^{T} \beta_{*} -\mathrm{E} x_{0}^{T} \hat{\beta}_{\alpha} \right]^{2} \\ &=\mathrm{E}_{x_{0}}\left[f\left(x_{0}\right)-x_{0}^{T} \beta_{*}\right]^{2}+\mathrm{E}_{x_{0}}\left[x_{0}^{T} \beta_{*}-\mathrm{E} x_{0}^{T} \hat{\beta}_{\alpha}\right]^{2} \\
&\left.=\text { Ave[Model Bias }]^{2}+\text { Ave[Estimation Bias }\right]^{2}
\end{aligned}
$$


- ì²«ë²ˆì§¸ term(Average squared model bias): best fitting íšŒê·€ì‹ì˜ ì¶”ì •ì¹˜ì™€ true function ì‚¬ì´ì˜ error
- ë‘ë²ˆì§¸ term(Average squared estimation bias): $E(x_0^T\hat{\beta}_a)$ì™€ best fitting íšŒê·€ì‹ ì¶”ì •ì¹˜ ì‚¬ì´ì˜ error

ì¼ë°˜ì ì¸ íšŒê·€ì‹ì˜ ê²½ìš° estimation biasê°€ 0ì¸ ë°˜ë©´ ridgeë‚˜ lassoì™€ ê°™ì€ ì œì•½ termì„ í¬í•¨í•œ íšŒê·€ì‹ì€ ì–‘ì˜ ê°’ì„ ê°–ëŠ”ë‹¤. í•˜ì§€ë§Œ bias-variance trade-off ê´€ê³„ì— ë”°ë¼ varianceê°€ ê°ì†Œí•œë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤. 



## 4. Optimism of the Training Error Rate

training set $\mathcal{T}=\{(x_1,y_1),(x_2,y_2),\cdots,(x_n,y_n)\}$ ì´ ì£¼ì–´ì¡Œì„ ë•Œ ëª¨í˜• $\hat{f}$ ì— ëŒ€í•œ **generalization error**ëŠ” $Err_{\mathcal{T}} = E_{X_0,Y_0}[L(Y_0,\hat{f}(X_0))\mid\mathcal{T}]$ ì´ë‹¤. Training setì€ ê³ ì •ëœ ìƒíƒœì´ë©° $(X_0,Y_0)$ëŠ” ìƒˆë¡œìš´ ë°ì´í„° í¬ì¸íŠ¸ë‹¤. ëª¨ë“  training setì— ëŒ€í•´ generalizaiton errorë¥¼ êµ¬í•´ ê¸°ëŒ€ê°’ì„ ì·¨í•˜ë©´ expected errorë¥¼ ì–»ì„ ìˆ˜ ìˆìœ¼ë©° í˜•íƒœëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.  


$$
E_{\mathcal{T}}E_{X_0,Y_0}[L(Y_0,\hat{f}(X_0))\mid\mathcal{T}]
$$


ì¼ë°˜ì ìœ¼ë¡œ training error $\frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, \hat{f}\left(x_{i}\right)\right)$ ëŠ” ëª¨í˜•ì„ ë§Œë“¤ ë•Œ ì‚¬ìš©í•œ ë°ì´í„°ë¥¼ ê°€ì§€ê³  errorë¥¼ êµ¬í•œ ê²ƒì´ê¸° ë•Œë¬¸ì— true error $Err_{\mathcal{T}}$ ë³´ë‹¤ ì‘ë‹¤. ë”°ë¼ì„œ training errorê°€ $Err_{\mathcal{T}}$ ì˜ optimistic estimateìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤. 



> In-sample error


$$
\operatorname{Err}_{\mathrm{in}}=\frac{1}{N} \sum_{i=1}^{N} \mathrm{E}_{Y^{0}}\left[L\left(Y_{i}^{0}, \hat{f}\left(x_{i}\right)\right) \mid \mathcal{T}\right]
$$



In-sample errorëŠ” train errorì˜ ê³¼ì†Œì¶”ì •ëœ ì •ë„ $op$ë¥¼ ë”í•´ì¤€ í†µê³„ëŸ‰ì´ë‹¤. $Y_i^0$ëŠ” ê° training point $x_i\ i=1,2,\cdots,n$ ì—ì„œì˜ new response Nê°œë¥¼ ì˜ë¯¸í•œë‹¤. ê°  $x_i\ (i=1,\cdots,N)$ì—ì„œì˜ new response Nê°œì™€ $\hat{f(x_i)}$ì˜ expected lossë¥¼ êµ¬í•´ í‰ê· ì„ ì·¨í•˜ë©´ in-sample errorë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤

$op$ëŠ” in-sample errorì™€ $Err_{in}$ì˜ ì°¨ì´ë¡œ ì •ì˜ëœë‹¤. ì¼ë°˜ì ìœ¼ë¡œ training errorëŠ” downward biased ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— $op$ëŠ” ì–‘ìˆ˜ë‹¤. training setì„ random variableë¡œ ë³´ê³  $\mathcal{T}$ ì— ëŒ€í•´ expectationì„ ì·¨í•˜ë©´ average optimism $\omega$ ì„ ì–»ì„ ìˆ˜ ìˆë‹¤. ìš°ë¦¬ì˜ ê´€ì‹¬ì‚¬ëŠ” $op$ ë¥¼ ì¶”ì •í•˜ëŠ” ê²ƒì´ì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ $\omega$ë¥¼ ì¶”ì •í•˜ëŠ” ê²ƒì´ ë” ì‰½ë‹¤


$$
op = Err_{in}-\overline{err}\quad \omega = E_y(op)
$$


> sqaured error, 0-1 loss function

sqaured error, 0-1 loss function ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ì— $\omega$ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì‰½ê²Œ êµ¬í•  ìˆ˜ ìˆë‹¤. 


$$
\omega = \frac{2}{N}\sum_{i=1}^N Cov(\hat{y}_i,y_i)
$$


## 10. Cross- validation

cross validationì€ expected prediction errorë¥¼ ì¶”ì •í•˜ëŠ” ê°€ì¥ ëŒ€í‘œì ì¸ ë°©ë²•ì´ë‹¤. 

### 10-1 K-fold CV

ë°ì´í„°ê°€ ì¶©ë¶„í•œ ê²½ìš°ì— ë°ì´í„°ë¥¼ train setê³¼ validation setìœ¼ë¡œ ë‚˜ëˆ  ì˜ˆì¸¡ ëª¨í˜•ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ”ë° ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œëŠ” ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— **k-fold cv** ë¥¼ ì‚¬ìš©í•œë‹¤. ë°ì´í„°ë¥¼ Kê°œì˜ ì˜ì—­ìœ¼ë¡œ ë‚˜ëˆ ì„œ K-1ê°œì˜ ë°ì´í„° ì…‹ì€ ëª¨ë¸ì„ fittingí•˜ëŠ”ë° ë‚˜ë¨¸ì§€ í•œê°œì˜ ë°ì´í„° ì…‹ì€ testí•˜ëŠ”ë° ì‚¬ìš©í•œë‹¤. K=5ì¸ ê²½ìš°ë¥¼ ë„ì‹í™”í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. 



![7](http://whdbfla6.github.io/assets/ml/7.2.PNG)



ë°ì´í„°ë¥¼ 5ê°œì˜ ì˜ì—­ìœ¼ë¡œ ë‚˜ëˆ„ê³  4ê°œì˜ ì˜ì—­ì— ì†í•œ ë°ì´í„°ë¡œ ëª¨ë¸ì„ fittingí•˜ê³  ë‚˜ë¨¸ì§€ ì˜ì—­ì˜ ë°ì´í„°ë¡œ prediction errorë¥¼ ê³„ì‚°í•œë‹¤. ì´ë¥¼ ëª¨ë“  ë°ì´í„° sectionì— ëŒ€í•´ ë°˜ë³µí•˜ë©´ 5ê°œì˜ ì¶”ì •ì¹˜ë¥¼ ì–»ì„ ìˆ˜ ìˆìœ¼ë©°, í‰ê· ì„ ë‚´ë©´ prediction errorì— ëŒ€í•œ ì¶”ì •ì¹˜ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.

ì¼ë°˜í™”ì‹œí‚¨ cross validation estimate of prediction errorëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. ì¼ë°˜ì ìœ¼ë¡œ kê°’ìœ¼ë¡œ 5ë‚˜ 10ì„ ì‚¬ìš©í•œë‹¤. 


$$
\mathrm{CV}(\hat{f})=\frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, \hat{f}^{-\kappa(i)}\left(x_{i}\right)\right)
$$


ë‹¤ìŒìœ¼ë¡œ Kê°’ì— ë”°ë¥¸ ì¶”ì •ì¹˜ì˜ bias-variance trade off ê´€ê³„ë¥¼ ì‚´í´ë³¼ ê²ƒì´ë‹¤

> 1. K=2



> 2. K=N LOOCV



> 3. K=5 



```html
<details>
<summary>ì¦ëª…</summary>
<div markdown="1">       

ğŸ˜ìˆ¨ê²¨ì§„ ë‚´ìš©ğŸ˜

</div>
</details>
```

## 11. Bootstrap Methods

