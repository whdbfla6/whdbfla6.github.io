---
title: '[ML] 7.Model Assesment and Selection'
use_math: true
comments: true
layout: single
classes: wide
categories:

  - 머신러닝
  - ESL

tags:
  
  - 머신러닝
  - ESL
---





이 단원에서는 모형 성능 평가와, 평가를 바탕으로 모형을 선택하는 방법에 대해 다룰 것이다. 

## 2. Bias, Variance and Model Complexity

input vector $X$ target variable $Y$ 그리고 train data로부터 추정된 $\hat{f}(X)$ 예측모델이 있다고 하자. **Loss function**은 실제 $Y$값과 모델로부터 예측한 $\hat{f}(X)$ 사이의 error를 나타낸다. Loss function $L(Y,\hat{f}(X))$ 을 계산하는 방법은 여러가지가 있는데 일반적으로 절대값의 차이나 차이의 제곱을 이용한다. 


$$
L(Y, \hat{f}(X))=\left\{\begin{array}{ll}
(Y-\hat{f}(X))^{2} & \text { squared error } \\
|Y-\hat{f}(X)| & \text { absolute error }
\end{array}\right.
$$


여기서 **Test error(Generalization error)**는 독립적인 test sample에 대한 prediction error다.  train sample $\mathcal{T}$ 은 주어진 상태이며, test error는 특정한 training set에 대한 error를 나타낸다


$$
\operatorname{Err}_{\mathcal{T}}=\mathrm{E}[L(Y, \hat{f}(X)) \mid \mathcal{T}]
$$


이번에는 train set이 고정된 것이 아니라 random하게 주어진 상황에서 기대값을 구해보자. 이를 **expected test error** 혹은 **expected prediction error**라 하며 다음과 같이 구할 수 있다. 


$$
\operatorname{Err}=\mathrm{E}[L(Y, \hat{f}(X))]=\mathrm{E}\left[\operatorname{Err}_{\mathcal{T}}\right]
$$


![7](http://whdbfla6.github.io/assets/ml/7.1.PNG)



이 그림에서 연한 red 커브는 100개 train set 각각에 대한 $Err_{\mathcal{T}}$ 을 나타낸다. 진한 red 커브는 100개의 $Err_{\mathcal{T}}$ 에 대한 기대값으로 expected prediction error다. 우리의 목표는 train set이 주어졌을 때 error에 대한 기대값을 구하는 것이지만 대부분의 방법론은 error의 기대값을 추정하고 있다. 

모델의 구조가 복잡해지는 경우 bias는 감소하지만 variance가 커지는 문제가 생긴다. 즉 새로운 데이터가 들어왔을 때 예측을 잘 못하는 것을 의미한다. bias와 variance를 모두 줄여 expected test error를 최소화하는 모델을 선택하는 것이 중요하다

**training error**는 모든 train sample에 대해 loss값을 계산해 평균을 구한 것이다


$$
\overline{\mathrm{err}}=\frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, \hat{f}\left(x_{i}\right)\right) .
$$


train error의 경우 모델의 구조가 복잡해질수록 작은 값을 가져 모델의 복잡도가 충분히 크다면(ex.데이터를 모두 연결한 function) 0의 값을 갖는다. 따라서 training error는 test error에 대한 추정치로 사용할 수 없다. 일반적으로는 데이터를 ```training set, validation set, test set```으로 분리해 training set은 모델을 fitting하는 경우 validation set은 모델 선택을 위해 prediction error를 추정하는 경우, test set은 최종적으로 선택한 모델에 대한 prediction error를 구하는데 사용된다. 여기서 모형 선택은 서로 다른 모형의 성능을 추정해 가장 좋은 성능을 가진 모형을 고르는 과정이며, 모형 평가는 모형을 선택한 후에 새로운 데이터에 대한 Generalization error를 추정하는 것을 의미한다.  일반적으로 전체 데이터 셋의 반을 training set 나머지의 1/2을 각각 validation set과 test set으로 사용한다. 하지만 대부분의 상황에서는 데이터의 양이 충분하지 않아 세 개의 파트로 나눌 수가 없다. 따라서 이후의 단원에서는 이러한 상황에서 모델 성능을 평가하는 방법에 대해 다룰 것이다. 



## 3. The Bias-Variance Decomposition



## 4. Optimism of the Training Error Rate



