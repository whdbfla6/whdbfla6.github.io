--- 
title: '[NLP]Lecture 13'
usemathjax: true
use_math: true
comments: true
layout: single
classes: wide
categories:
  - NLP

---

본 글은 스탠포드 [CS 224N](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/) [Contextual word embeddings] 강의를 들으며 정리한 글 입니다. ELMO, BERT의 내용을 담고 있습니다. 

## ELMO

### 1. ELMO란?

ELMO는 2018년도에 새롭게 제안된 워드 임베딩 방법이다. ELMO는 `Embeddings from Language Models`의 약자로 Pre-trained된 언어모델을 사용했다는 큰 특징이 있다. ELMO는 모델을 pre-trained 한 뒤에 분류, Q&A와 같은 Task에 맞게 업데이트하는 과정을 거친다. 이를 fine-tuning이라 한다. 

### 2. ELMO 구조

언어모델을 pre-train하여 습득한 의미적 문법적인 관계 정보를 활용해서 다양한 테스크를 빠르게 습득할 수 있다. ELMO 구조는 크게 3가지 부분으로 구성된다. 

- 1) CNN을 활용한 문자 단위의 임베딩 : 각 단어 내 문자들 사이의 의미적,문법적 관계 도출
- 2) Bi-directional LSTM : 단어 사이의 문법적, 의미적 관계를 추출
- 3) ELMO layer : pre-train 후에 분류, Q&A와 같은 Task를 수행하는 과정에서 학습됨

![](https://i.imgur.com/mfHGfHS.png)


1)을 통해 구한 워드 임베딩이 양방향 LSTM의 인풋값으로 들어간다. 2)에서 각 단어의 위치에 해당하는 순방향/역방향에서 얻은 출력값 3개를 각각 concat하여 3)의 인풋으로 들어간다. 

### 2-1 pre-train 과정

#### 1) CNN을 활용한 문자 단위의 임베딩

![](https://i.imgur.com/EgS4B68.png)

> input
    
ELMO의 경우 Input으로 character 그대로 넣는 것이 아니라, 문자 양 끝에 단어의 시작과 끝을 알리는 start, end토큰을 부여한다. 또한 문자의 최대값(max-character)에 맞춰서 오른쪽 끝에 제로패딩을 채워준다.

> Filter,pooling

컨볼루션 필터의 경우 size1 32개, size2 32개,..., size7 1024개를 사용하며, 각각의 feature-map에 풀링 작업을 한다. 7종류의 필터를 사용하기 때문에 풀링 벡터 또한 7개가 나온다. 최종적인 output은 7개의 벡터를 concat한 형태이며, 벡터의 차원은 아래와 같다.

$$32+32+64+128+256+512+1024=2048$$

> 하이웨이 네트워크 
    
컨볼루션 레이어를 통과한 후에는 벡터의 차원이 2048로 매우 크기 때문에 하이웨이 네트워크를 통해 차원의 수를 조정해준다. 이로써 각 단어에 해당하는 임베딩이 탄생하고, 이 임베딩을 양방향 lstm의 인풋 값으로 들어가게 된다. 

[하이웨이 네트워크](https://whdbfla6.github.io/nlp/NLP-Lecture12/)에 대한 자세한 설명은 이 링크에 있다.

#### 2) Bi-directional LSTM 

![](https://miro.medium.com/max/875/1*ko2Ut74J_oMxF4jSo1VnCg.png)

문자 단위 컨볼루션 신경망을 통과한 각 단어 벡터들은 두개의 LSTM 레이어의 인풋값으로 들어간다. 하나는 순방향 LSTM 레이어 다른 하나는 역방햔 LSTM 레이어다. 

엘모 모델이 PRE-TRAIN할 때는 단어가 주어졌을 때 다음 단어가 무엇인지 맞춰야한다. 이 과정을 반복하다 보면 단어들 사이의 의미적 문법적 관계를 이해할 수 있게 된다. 
    
> train 방법
    
lstm 레이어의 상단 셀의 출력 히든 벡터를 선형변환한 뒤에 소프트맥스를 취한다. 이 확률 벡터와 정답벡터인 one-hot vector로 cross-entropy를 계산한다. 이 loss를 최소화하는 방식으로 파라미터를 업데이트한다. elmo의 경우 소프트맥스 함수를 적용할 때 corpus에 속한 10만개의 어휘집단을 모두 이용하는 것이 아니라, 일부 단어들만 샘플링한다. 즉 positive 샘플과 negative 샘플을 이용해 소프트맥스 확률을 계산한다. 


### 2-2 ELMO 구조 - fine-tuning

#### 1) ELMO layer

> bi-LSTM 활용 
    
![](https://wikidocs.net/images/page/33930/playwordvector.PNG)

BI-LSTM의 학습이 종료되면, 단어가 위치한 각 층의 출력값을 이용해 워드 임베딩을 구축한다. 이 모델에서는 1) 문자 단위 컨볼루션 출력 벡터 2) 첫번째 레이어의 출력벡터 3) 두번째 레이어의 출력벡터 3종류의 벡터가 순방향 역방향 모델에서 두개씩 나온다. 이 6개의 벡터를 이용해서 각 단어의 임베딩을 얻는 과정은 아래와 같다. 

- 각 층의 출력값을 연결한다. 

![](https://wikidocs.net/images/page/33930/concatenate.PNG)

- 각 층의 출력값 별로 가중치를 준다 

![](https://wikidocs.net/images/page/33930/weight.PNG)

- 각 층의 출력값을 모두 더한다

![](https://wikidocs.net/images/page/33930/weightedsum.PNG)

- 테스크 수행에 적합한 $\gamma^{task}$를 곱해준다

![](https://wikidocs.net/images/page/33930/scalarparameter.PNG)

입력 문장 k번째 토큰에 대한 ELMO 임베딩을 구하는 과정을 수식으로 표현하면 다음과 같다.

$$ELMO_k =\gamma^{task}\sum_{j=0}^2s_jh_{k,j} $$

이렇게 구축된 임베딩을 **ELMO 표현**이라고 한다. 

> task 수행

![](https://wikidocs.net/images/page/33930/elmorepresentation.PNG)

위에서 구한 엘모 표현은 텍스트 분류와 같은 특정 Task를 수행하는데 사용된다. ELMO표현은 기존의 임베딩벡터를 사용할 수 있다. GLOVE로 학습한 임베딩 벡터를 사용한다고 하면 ELMO표현을 GLOVE 임베딩 벡터와 연결해서 입력으로 사용된다. pre-trained된 언어모델에서 사용한 가중치들은 고정하며 $s,\gamma^{task}$는 훈련 과정에서 학습된다. 

## 참고문헌
1. [cnn char-level 그림](https://arxiv.org/pdf/1602.02410.pdf)
2. [bi-lstm](https://medium.com/saarthi-ai/elmo-for-contextual-word-embedding-for-text-classification-24c9693b0045)
3. [엘모 layer](https://wikidocs.net/33930)
