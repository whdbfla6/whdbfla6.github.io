---
title: '[ML] 4. Linear Methods for Classification'
use_math: true
comments: true
layout: single
classes: wide
categories:

  - 머신러닝
  - ESL

---

## 2. Linear Regression of an Indicator Matrix

여기서는 분류를 위해 각 response variable을 **Indicator variable**로 둔다. K번째 클래스에 해당하는 반응변수는 k번째 성분은 1 나머지는 0인 $1\times K$ 길이의 벡터다.  N개의 데이터를 모두 포함한 Indicator response matrix $Y$는 $N \times K$ 크기의 행렬로, 0과 1의 값만 갖는다. p개의 coefficients들이 있다고 할 때, $X$ 는 상수 term 1을 포함한 $N\times (p+1)$ 크기의 행렬이 되며, linear regression에 fitting한 결과 $\hat{Y}$ 은 다음과 같이 구할 수 있다. 
$$
Y = \begin{bmatrix}{} 0 \\ \vdots \\1 \\ \vdots \\0 \end{bmatrix}= \begin{bmatrix}{} Y_1 \\ \vdots \\Y_k \\ \vdots \\Y_K \end{bmatrix}\quad X: N \times (p+1)\ matrix\quad \hat{B} = (X^TX)^{-1}X^TY\quad \hat{Y} = X\hat{B}=X(X^TX)^{-1}X^TY
$$
새로운 관측지 $x$ 에 대한 $\hat{f(x)}$ 추정치는 $(1,x^T)\hat{B}$  로 길이가 K인 벡터이며, k번째 원소가 가장 큰 값을 가질 때 k번째 클래스로 분류가 된다. 

$Y_k$에 대해 $E(Y_K|X=x) = P(G=k|X=x)$ 이기 때문에, 각 추정치는 X값이 주어질 때 k 클래스에 속할 확률을 의미한다. 여기서 각 추정치는 확률에 대한 좋은 추정치일까? 확률값이 되기 위해서는 각 추정치는 0과 1사이의 값을 가져야하고, 모든 추정치의 합이 1이 되어야 한다. 하지만 linear regression의 특성상 train data의 바깥 범위에 대한 추정치는 음수 혹은 1보다 큰 값을 가질 수 있어 확률의 속성을 위반한다. 이 경우에 basis expansion을 통해 확률에 대한 consistent한 추정치를 얻을 수 있는데, 이후에 배울 로지스틱 회귀가 대표적인 예다.



## 3. Linear Discriminant Analysis



### 3.2 Computations for LDA



### 3.3 Reduced Rank Linear Discriminant Analysis 



## 4. Logistic Regression



### 4.3 Quadratic Approximations and Inferences 



## 5. Separating Hyperplanes



