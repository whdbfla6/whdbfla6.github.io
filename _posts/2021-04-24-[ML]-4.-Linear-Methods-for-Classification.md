---
title: '[ML] 4. Linear Methods for Classification'
use_math: true
comments: true
layout: single
classes: wide
categories:

  - 머신러닝
  - ESL

tags:
  
  - 머신러닝
  - ESL

---

Linear Regression of an Indicator matrix

## 2. Linear Regression of an Indicator Matrix

이 방법론은 분류를 위해 각 response variable을 **Indicator variable**로 둔다. K번째 클래스에 해당하는 반응변수는 k번째 성분은 1 나머지는 0인 $1\times K$ 길이의 벡터다.  N개의 데이터를 모두 포함한 Indicator response matrix $Y$는 $N \times K$ 크기의 행렬로, 0과 1의 값만 갖는다. p개의 coefficients들이 있다고 할 때, $X$ 는 상수 term 1을 포함한 $N\times (p+1)$ 크기의 행렬이 되며, linear regression에 fitting해 $\hat{Y}$ 을 구할 수 있다. 



$$
Y = \begin{bmatrix}{} 0 \\ \vdots \\1 \\ \vdots \\0 \end{bmatrix}= \begin{bmatrix}{} Y_1 \ \vdots \\Y_k \\ \vdots \\Y_K \end{bmatrix}\quad X: N \times (p+1)\ matrix\quad \hat{B} = (X^TX)^{-1}X^TY\quad \hat{Y} = X\hat{B}=X(X^TX)^{-1}X^TY\\ X: N\times (p+1)\quad \hat{\beta}:(p+1)\times K\quad Y : N\times K
$$



새로운 관측지 $x$ 에 대한 $\hat{f(x)}$ 추정치는 $(1,x^T)\hat{B}$  로 길이가 K인 벡터이며, k번째 원소가 가장 큰 값을 가질 때 k번째 클래스로 분류가 된다. 


$$
\hat{f(x)} = (1,x^T)\hat{B}\quad \hat{G}(x)=argmax_{k\in G}\hat{f}(x)
$$



회귀식은 $E(Y_K\mid X=x)$ 을 추정하는 것인데, $Y_k$ 에 대해서 $E(Y_K\mid X=x)=P(Y_K\mid X=x)$ 로 바라볼 수 있다. 즉 X값이 주어질 때 k 클래스에 속할 확률을 추정하는 것이다. 여기서 각 추정치는 확률에 대한 좋은 추정치일까? 확률값이 되기 위해서는 각 추정치는 0과 1사이의 값을 가져야하고, 모든 추정치의 합이 1이어야 한다.



> $\sum{\hat{f}_k(x)} = 1$



$$
\begin{bmatrix} \sum{\hat{f}(x)} \\ \vdots \\ \sum{\hat{f}(x)} \end{bmatrix} = \begin{bmatrix} \hat{Y}_{11} & \cdots & \hat{Y}_{1n} \\ \vdots & & \vdots \\ \hat{Y}_{n1} & \cdots & \hat{Y}_{nn} \end{bmatrix} \begin{bmatrix} 1 \\ \vdots \\ 1\end{bmatrix} = X(X^TX)X^TY\mathbf{1_n} = H\mathbf{1_n} =\mathbf{1_n}
$$



여기서 $X(X^TX)X^T$는 projection matrix로 $X(X^TX)X^TY\mathbf{1_n}$ 은  $Y\mathbf{1_n}$ 을 X의 column space에 projection하는 것을 의미한다. $Y$는 $N\times K$ 크기의 행렬이며 각 행은 하나의 값은 1 나머지는 모두 0의 값을 가지기 때문에 $Y\mathbf{1_n}$ 은 $\mathbf{1_n}$과 동일하다. 여기서 $X$ intercept term인 $\mathbf{1_n}$ 을 이미 포함하고 있기 때문에 projection한 결과도 $\mathbf{1_n}$ 이다. 추정치의 합이 1이라는 것에 대한 증명이 끝났다



> $0 \le \hat{f}(X) \le 1$

linear regression의 특성상 train data의 바깥 범위에 대한 추정치는 음수 혹은 1보다 큰 값을 가질 수 있어 확률의 속성을 위반하게 된다. 이 경우에 basis expansion을 통해 확률에 대한 consistent한 추정치를 얻을 수 있는데, 이후에 배울 로지스틱 회귀가 대표적인 예다.

회귀식 접근방식의 또다른 문제점은 class가 3개 이상 존재하는 경우에 특정 클래스가 다른 클래스에 가려져 완벽하게 분류가 되지 않는다는 점이다. $p=2$이고 클래스가 3개인 다음 예시를 살펴보자.



<p align = "center"><img src="http://whdbfla6.github.io/assets/ml/4.1.png" alt="4.1" style="zoom:80%;"  /> </p>



이 경우 클래스 2에 대한 회귀식은 수평선 형태를 갖기 때문에 모든 데이터는 클래스1 혹은 클래스3으로 분류가 된다. 이번에는 linear한 형태가 아닌 quadratic term을 추가해서 fitting을 해보자.



<p align = "center"><img src="http://whdbfla6.github.io/assets/ml/4.2.PNG" alt="4.1" style="zoom:80%;"  /> </p>



2차항을 포함해 fitting한 결과 클래스 2 또한 분류가 잘되고 있음을 확인할 수 있다. 

## 3. Linear Discriminant Analysis

분류에서 우리의 관심사는 데이터가 주어졌을 때 클래스 $k$에 속할 확률을 구하는 것이다. 베이즈 정리에 따르면 class posterior는 다음과 같다


$$
P(G=K\mid X=x) = \frac{f_k(x)\pi_k}{\sum_{l=1}^Kf_l(x)\pi_l}
$$



1. $\pi_k$ : 클래스 k의 사전확률로 $\sum_{l=1}^K\pi_l=1$이 되어야 한다
2. $f(x)$는 클래스 k에 속한 $X$의 확률분포다. Linear discriminant analysis 와 Quadratic discriminant analysis는 다변량 정규분포를 가정하고 있다.



$$
f_{k}(x)=\frac{1}{(2 \pi)^{p / 2}\left|\Sigma_{k}\right|^{1 / 2}} e^{-\frac{1}{2}\left(x-\mu_{k}\right)^{T} \Sigma_{k}^{-1}\left(x-\mu_{k}\right)}
$$



여기서 LDA는 가정사항을 하나 더 포함하고 있는데 클래스 별 covariance matrix가 동일하다고 가정한다.  따라서 클래스 $k$와 $l$의 log poserior odds는 다음과 같다



$$
\begin{array}{c}
\log \frac{\operatorname{Pr}(G=k \mid X=x)}{\operatorname{Pr}(G=\ell \mid X=x)}=\log \frac{f_{k}(x)}{f_{\ell}(x)}+\log \frac{\pi_{k}}{\pi_{\ell}} \\
=\log \frac{\pi_{k}}{\pi_{\ell}}-\frac{1}{2}\left(\mu_{k}+\mu_{\ell}\right)^{T} \Sigma^{-1}\left(\mu_{k}-\mu_{\ell}\right) +x^{T} \Sigma^{-1}\left(\mu_{k}-\mu_{\ell}\right)
\end{array}
$$



covariance matrix가 동일하다고 가정하고 있기 때문에 normalizing constant가 약분이 되고 exponent 안에 들어가는 quadratic part가 사라지게 된다. 따라서 클래스 $k$와 $l$을 분류하는 decision boundary $p(G=k\mid X=x)=p(G=l\mid X=x)$는 x에 linear한 형태다. 즉 $R^P$공간을 $K$개의 hyperplane으로 분리되는 것이다. 아래 그림은 클래스가 3개이고 $p=2$인 경우에 해당된다. 



<p align = "center"><img src="http://whdbfla6.github.io/assets/ml/4.3.PNG" alt="4"  /> </p>



linear discriminant function은 클래스를 구분지어주는 함수로 $k$와 관련된 term만 남아 다음과 같이 구성된다. 최종적으로 ldf 값이 가장 크게 나오는 클래스 k로 분류가 된다.

$$
\delta_{k}(x)=x^{T} \mathbf{\Sigma}^{-1} \mu_{k}-\frac{1}{2} \mu_{k}^{T} \mathbf{\Sigma}^{-1} \mu_{k}+\log \pi_{k}
$$


여기서 정규분포의 파라미터들을 추정해야 하는데, 추정치는 다음과 같다.


$$
\begin{array}{l}
\hat{\pi}_{k}=N_{k} / N, \text { where } N_{k} \text { is the number of class- } k \text { observations }\\
\hat{\mu}_{k}=\sum_{g_{i}=k} x_{i} / N_{k}\\
\hat{\mathbf{\Sigma}}=\sum_{k=1}^{K} \sum_{q_{i}=k}\left(x_{i}-\hat{\mu}_{k}\right)\left(x_{i}-\hat{\mu}_{k}\right)^{T} /(N-K) .
\end{array}
$$


일반적으로 각 클래스에 대한 사전확률은 갖고 있는 데이터의 클래스 비율이다. 평균벡터는 MLE를 사용하고 있으며, 공분산행렬의 경우 pooled covariance를 사용한다. 

QDA의 경우 <u>각 클래스 별로 Covariance matrix가 다르다고 가정</u>한다. 따라서 위와같이 약분이 되지 않고, quadratic term이 남게 된다. quadratic discriminant function 형태는 다음과 같다.


$$
\delta_{k}(x)=-\frac{1}{2} \log \left|\Sigma_{k}\right|-\frac{1}{2}\left(x-\mu_{k}\right)^{T} \mathbf{\Sigma}_{k}^{-1}\left(x-\mu_{k}\right)+\log \pi_{k}
$$

$k$와 $l$의 decision boundary는 quadratic discriminant function이 같아지는 x들의 집합으로 아래와 같이 곡선 형태다. 



<p align = "center"><img src="http://whdbfla6.github.io/assets/ml/4.4.PNG" alt="4"  /> </p>



### 3.3 Reduced Rank Linear Discriminant Analysis 



## 4. Logistic Regression

logistic regression은 decision boundary를 x에 linear한 function으로 모델링하고자 한다. 앞서 Y를 indicator function으로 구성해 linear function을 fitting하는 경우 각 추정치가 확률이 아닌 값을 가진다는 문제점이 있었다. 로지스틱 회귀의 경우 사후확률의 총합이 1이고 각 추정치들이 0과 1사이의 값을 가져 확률의 조건을 만족한다. 로지스틱 회귀는 log odds를 다음과 같이 구성한다.


$$
\begin{array}{c}
\log \frac{\operatorname{Pr}(G=1 \mid X=x)}{\operatorname{Pr}(G=K \mid X=x)}=\beta_{10}+\beta_{1}^{T} x \\
\log \frac{\operatorname{Pr}(G=2 \mid X=x)}{\operatorname{Pr}(G=K \mid X=x)}=\beta_{20}+\beta_{2}^{T} x \\
\vdots \\
\log \frac{\operatorname{Pr}(G=K-1 \mid X=x)}{\operatorname{Pr}(G=K \mid X=x)}=\beta_{(K-1) 0}+\beta_{K-1}^{T} x
\end{array}
$$


모델은 $K-1$개의 log odds로 구성되며, 일반적으로 마지막 클래스의 사후확률을 log odds의 분모로 사용한다. 위 식을 계산해 정리하면 각 클래스의 사후확률은 다음과 같이 나온다. 식의 형태를 보면 각 클래스에 속할 확률의 총 합이 1이고, 각 사후확률 값이 0과 1사이의 범위에 속하는 것을 확인할 수 있다.


$$
\begin{array}{l}
\operatorname{Pr}(G=k \mid X=x)=\frac{\exp \left(\beta_{k 0}+\beta_{k}^{T} x\right)}{1+\sum_{\ell=1}^{K-1} \exp \left(\beta_{\ell 0}+\beta_{\ell}^{T} x\right)}\quad k=1, \ldots, K-1, \\
\operatorname{Pr}(G=K \mid X=x)=\frac{1}{1+\sum_{\ell=1}^{K-1} \exp \left(\beta_{\ell 0}+\beta_{\ell}^{T} x\right)}
\end{array}
$$



### 4.1 Fitting Logistic Regression Models

일반적으로 로지스틱회귀의 베타값은 **maximum likelihood estimator**로 추정된다. $X$는 $K$ 클래스 중 하나에 무조건 속하며 $p(G\mid X)$가 주어진 상태이기 때문에 multinomial distribution을 따른다고 가정한다. 그 가정 아래 $N$개의 관측치에 대한 log-likelihood는 다음과 같이 나타낼 수 있다. 


$$
\ell(\theta)=\sum_{i=1}^{N} \log p_{g_{i}}\left(x_{i} ; \theta\right)\quad p_k(x_i;\theta)= p(G=k\mid X_i=x_i; \theta)
$$


클래스가 두개인 경우에 대해서 살펴보자. 1번 클래스에 속할 확률이 $p(x;\theta)$인 경우에 클래스 2에 속할 확률은 $1-p(x;\theta)$ 이다. $y_i$ 값이 클래스 1에 속하는 경우 1, 클래스 2에 속하는 경우 0의 값을 갖는다고 하면, log-likelihood는 다음과 같이 쓸 수 있다


$$
\ell(\beta)=\sum_{i=1}^{N}\left\{y_{i} \log p\left(x_{i} ; \beta\right)+\left(1-y_{i}\right) \log \left(1-p\left(x_{i} ; \beta\right)\right)\right\}
$$


$p(x_i;\beta) = \frac{e^{\beta^TX}}{1+e^{\beta^TX}}$ 값을 대입해 정리를 하면 log-likelihood는 다음과 같이 간단히 정리된다.


$$
\ell(\beta)= \sum_{i=1}^{N}\left\{y_{i} \beta^{T} x_{i}-\log \left(1+e^{\beta^{T} x_{i}}\right)\right\}
$$


현재 $\beta$는 $\beta_{10},\beta_{1}$ 즉 intercept term을 포함하고 있기 때문에 $x_i$ 또한 1을 포함하고 있다고 생각하면 된다. log-likelihood를 최대로 만들어주는 베타값을 찾기 위해서는 미분을 해서 0으로 만들어주는 $\beta$를 찾으면 된다. 이 경우에 closed form을 찾는 것이 어렵기 때문에 **Newton-Raphson method**를 이용한다. 


$$
\frac{\partial \ell(\beta)}{\partial \beta}=\sum_{i=1}^{N} x_{i}\left(y_{i}-p\left(x_{i} ; \beta\right)\right)=0
$$


> Newton-Raphson method

Newton-Rahpson은 함수의 형태가 <u>convex한 경우에 함수의 최소 지점을 찾아주는 알고리즘</u> 중 하나다. 이 방법은 현시점 $x$에서 접선을 그리고, 해당 접선이 x축과 만나는 지점으로 이동하면서 해를 찾아가는 방법이다.



<p align = "center"><img src="http://whdbfla6.github.io/assets/ml/4.5.png" alt="4"  /> </p>



$x_1$지점에서의 접선은 $y=f(x_1)(x-x_1)+f(x_1)$ 이며 x축과 만나는 지점을 찾기 위해서는 $y$에 0을 대입하면 된다. y값에 0을 대입해 해를 찾으면 $x_2 = x_1 - \frac{f(x_1)}{f^\prime(x_1)}$  이다. 이 과정을 반복해서 해를 찾아가는 것이다. 


$$
x_{n+1} = x_{n} - \frac{f(x_n)}{f^\prime(x_n)}
$$


로지스틱 회귀에서는 $l^\prime(\beta)$를 최소화하는 $\beta$를 찾아야하기 때문에 $l^{\prime\prime}(\beta)$ 값이 필요하며, 다음과 같이 업데이트가 진행된다. 


$$
\frac{\partial^{2} \ell(\beta)}{\partial \beta \partial \beta^{T}}=-\sum_{i=1}^{N} x_{i} x_{i}^{T} p\left(x_{i} ; \beta\right)\left(1-p\left(x_{i} ; \beta\right)\right)\quad p\times p \\ \beta^{\text {new }}=\beta^{\text {old }}-\left(\frac{\partial^{2} \ell(\beta)}{\partial \beta \partial \beta^{T}}\right)^{-1} \frac{\partial \ell(\beta)}{\partial \beta},
$$


지금까지의 과정을 matrix form으로 나타내보자. $\mathbf{y}$ 가 $y_i$를 원소로 갖는 $N\times1$ 벡터, $\mathbf{X}$ 가 $N\times (p+1)$ matrix, $\mathbf{p}$가 $p(x_i;\beta^{old})$를 원소로 갖는 $N\times1$ 벡터, $\mathbf{W}$ 가 $p(x_i;\beta^{old})(1-p(x_i;\beta^{old}))$ 를 대각성분으로 갖는 $N\times N$ 행렬이라고 하자. 그럼 다음과 같이 나타낼 수 있다. 


$$
\begin{aligned}
\frac{\partial \ell(\beta)}{\partial \beta} &=\mathbf{X}^{T}(\mathbf{y}-\mathbf{p}) \\
\frac{\partial^{2} \ell(\beta)}{\partial \beta \partial \beta^{T}} &=-\mathbf{X}^{T} \mathbf{W X}
\end{aligned}
$$


위 식을 이용해 Newton's step을 나타내면 다음과 같으며 $\mid \beta^{new}-\beta^{old}\mid$의 값이 매우 작은 값을 가질 때까지 반복된다.


$$
\begin{aligned}
\beta^{\text {new }} &=\beta^{\text {old }}+\left(\mathbf{X}^{T} \mathbf{W X}\right)^{-1} \mathbf{X}^{T}(\mathbf{y}-\mathbf{p}) \\
&=\left(\mathbf{X}^{T} \mathbf{W} \mathbf{X}\right)^{-1} \mathbf{X}^{T} \mathbf{W}\left(\mathbf{X} \beta^{\text {old }}+\mathbf{W}^{-1}(\mathbf{y}-\mathbf{p})\right) \\
&=\left(\mathbf{X}^{T} \mathbf{W} \mathbf{X}\right)^{-1} \mathbf{X}^{T} \mathbf{W} \mathbf{z}
\end{aligned}
$$


여기서 $\mathbf{X} \beta^{\text {old }}+\mathbf{W}^{-1}(\mathbf{y}-\mathbf{p})$ 를 adjusted response $\mathbf{z}$로 본다면  $\beta^{new}$를 구하는 것은 반복적으로 weigted least square 문제를 푸는 것과 동일하다. 


$$
\beta^{\text {new }} \leftarrow \arg \min _{\beta}(\mathbf{z}-\mathbf{X} \beta)^{T} \mathbf{W}(\mathbf{z}-\mathbf{X} \beta)
$$


## 5. Separating Hyperplanes


$$
f(x) = \beta_0+\beta^T x = 0
$$


perceptron 알고리즘을 설명하기에 앞서 hyperplane $\mathbf{L}$의 성질 몇가지를 살펴볼 것이다.



<p align = "center"><img src="http://whdbfla6.github.io/assets/ml/4.6.PNG" alt="4" style="zoom:80%;"  /> </p>

1. hyperplane에 놓인 점 $x_1,x_2$ 가 있다고 하면 $\beta(x_1-x_2)=0$을 만족하게 된다. 두개의 벡터의 내적값이 0이기 때문에 $\beta^{*}=\beta /\|\beta\|$ 벡터는 $\mathbf{L}$ 에 수직이다.
2. $\mathbf{L}$에 놓인 임의의 점 $x_0$에 대해 $\beta^Tx_0=-\beta_0$ 를 만족한다

3. 임의의 점 $x$에서 $\mathbf{L}$까지의 거리는 $f(x)$에 비례한다

임의의 점 $x$에서 $\mathbf{L}$까지의 거리는 $x-x_0$을 벡터 $\beta$에 정사영한 것과 동일하다


$$
\begin{aligned}
\beta^{* T}\left(x-x_{0}\right) &=\frac{1}{\|\beta\|}\left(\beta^{T} x+\beta_{0}\right) \\
&=\frac{1}{\left\|f^{\prime}(x)\right\|} f(x) .
\end{aligned}
$$



### 5-1. Rosenblatt’s Perceptron Learning Algorithm

perceptron learning 알고리즘은 <u>오분류된 데이터와 decision boundary 까지의 거리를 최소화</u>하는 방식으로 hyperplane을 설정한다. 이 알고리즘은 $X^T\beta + \beta_0$ 값이 양수인 경우 $y_i=1$ 음수인 경우 $y_i=-1$ 의 값을 갖기 때문에 $y_i=1$에 해당하는 $x_i$가 오분류가 된 경우 $X^T\beta + \beta_0$ 가 음수가 나온다. 따라서 최종적인 목표는 $\mathbf{D}(\beta,\beta_0)$ 을 최소화하는 것이다. 오분류가 되는 경우에 $-y_i(X^T\beta + \beta_0)$ 가 양수이기 때문에 $\mathbf{D}(\beta,\beta_0)$ 을 최소화하는 것이 합리적이다.



$$
D\left(\beta, \beta_{0}\right)=-\sum_{i \in \mathcal{M}} y_{i}\left(x_{i}^{T} \beta+\beta_{0}\right),\quad \mathcal{M}: \text{missclassified point의 집합}
$$



이 알고리즘의 경우 stochastic gradient descent 방법을 사용해서 $\beta$를 업데이트 한다. 즉 모든 데이터를 사용해 loss function을 구해서 업데이트를 하는 것이 아니라 오분류된 관측치가 발견될 때마다 업데이트를 하는 것이다. $\rho$는 learning rate이며 일반적으로 1을 사용한다.



$$
\left(\begin{array}{c}
\beta \\
\beta_{0}
\end{array}\right) \leftarrow\left(\begin{array}{c}
\beta \\
\beta_{0}
\end{array}\right)+\rho\left(\begin{array}{c}
y_{i} x_{i} \\
y_{i}
\end{array}\right)
$$



이 방법론에도 여러가지 문제점이 존재한다.

- 분류가 가능한 경우에 다양한 solution이 존재하며, 초기값을 어떤 값으로 사용하는지에 따라 hyperplane이 결정된다
- 분류가 불가능한 경우에 수렴하지 않고 계속 반복된다
