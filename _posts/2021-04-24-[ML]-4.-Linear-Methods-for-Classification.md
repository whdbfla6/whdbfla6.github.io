---
title: '[ML] 4. Linear Methods for Classification'
use_math: true
comments: true
layout: single
classes: wide
categories:

  - 머신러닝
  - ESL

tags:
  
  - 머신러닝
  - ESL

---

Linear Regression of an Indicator matrix

## 2. Linear Regression of an Indicator Matrix

이 방법론은 분류를 위해 각 response variable을 **Indicator variable**로 둔다. K번째 클래스에 해당하는 반응변수는 k번째 성분은 1 나머지는 0인 $1\times K$ 길이의 벡터다.  N개의 데이터를 모두 포함한 Indicator response matrix $Y$는 $N \times K$ 크기의 행렬로, 0과 1의 값만 갖는다. p개의 coefficients들이 있다고 할 때, $X$ 는 상수 term 1을 포함한 $N\times (p+1)$ 크기의 행렬이 되며, linear regression에 fitting해 $\hat{Y}$ 을 구할 수 있다. 



$$
Y = \begin{bmatrix}{} 0 \\ \vdots \\1 \\ \vdots \\0 \end{bmatrix}= \begin{bmatrix}{} Y_1 \ \vdots \\Y_k \\ \vdots \\Y_K \end{bmatrix}\quad X: N \times (p+1)\ matrix\quad \hat{B} = (X^TX)^{-1}X^TY\quad \hat{Y} = X\hat{B}=X(X^TX)^{-1}X^TY\\ X: N\times (p+1)\quad \hat{\beta}:(p+1)\times K\quad Y : N\times K
$$



새로운 관측지 $x$ 에 대한 $\hat{f(x)}$ 추정치는 $(1,x^T)\hat{B}$  로 길이가 K인 벡터이며, k번째 원소가 가장 큰 값을 가질 때 k번째 클래스로 분류가 된다. 


$$
\hat{f(x)} = (1,x^T)\hat{B}\quad \hat{G}(x)=argmax_{k\in G}\hat{f}(x)
$$



회귀식은 $E(Y_K\mid X=x)$ 을 추정하는 것인데, $Y_k$ 에 대해서 $E(Y_K\mid X=x)=P(Y_K\mid X=x)$ 로 바라볼 수 있다. 즉 X값이 주어질 때 k 클래스에 속할 확률을 추정하는 것이다. 여기서 각 추정치는 확률에 대한 좋은 추정치일까? 확률값이 되기 위해서는 각 추정치는 0과 1사이의 값을 가져야하고, 모든 추정치의 합이 1이어야 한다.



> $\sum{\hat{f}_k(x)} = 1$



$$
\begin{bmatrix} \sum{\hat{f}(x)} \\ \vdots \\ \sum{\hat{f}(x)} \end{bmatrix} = \begin{bmatrix} \hat{Y}_{11} & \cdots & \hat{Y}_{1n} \\ \vdots & & \vdots \\ \hat{Y}_{n1} & \cdots & \hat{Y}_{nn} \end{bmatrix} \begin{bmatrix} 1 \\ \vdots \\ 1\end{bmatrix} = X(X^TX)X^TY\mathbf{1_n} = H\mathbf{1_n} =\mathbf{1_n}
$$



여기서 $X(X^TX)X^T$는 projection matrix로 $X(X^TX)X^TY\mathbf{1_n}$ 은  $Y\mathbf{1_n}$ 을 X의 column space에 projection하는 것을 의미한다. $Y$는 $N\times K$ 크기의 행렬이며 각 행은 하나의 값은 1 나머지는 모두 0의 값을 가지기 때문에 $Y\mathbf{1_n}$ 은 $\mathbf{1_n}$과 동일하다. 여기서 $X$ intercept term인 $\mathbf{1_n}$ 을 이미 포함하고 있기 때문에 projection한 결과도 $\mathbf{1_n}$ 이다. 추정치의 합이 1이라는 것에 대한 증명이 끝났다



> $0 \le \hat{f}(X) \le 1$

linear regression의 특성상 train data의 바깥 범위에 대한 추정치는 음수 혹은 1보다 큰 값을 가질 수 있어 확률의 속성을 위반하게 된다. 이 경우에 basis expansion을 통해 확률에 대한 consistent한 추정치를 얻을 수 있는데, 이후에 배울 로지스틱 회귀가 대표적인 예다.

회귀식 접근방식의 또다른 문제점은 class가 3개 이상 존재하는 경우에 특정 클래스가 다른 클래스에 가려져 완벽하게 분류가 되지 않는다는 점이다. $p=2$이고 클래스가 3개인 다음 예시를 살펴보자.

![4.1](http://whdbfla6.github.io/assets/ml/4.1.png)이 경우클래스 2에 대한 회귀식은 수평선 형태를 갖기 때문에 모든 데이터는 클래스1 혹은 클래스3으로 분류가 된다. 이번에는 linear한 형태가 아닌 quadratic term을 추가해서 fitting을 해보자.

![4.1](http://whdbfla6.github.io/assets/ml/4.2.PNG)

2차항을 포함해 fitting한 결과 클래스 2 또한 분류가 잘되고 있음을 확인할 수 있다. 

## 3. Linear Discriminant Analysis

분류에서 우리의 관심사는 데이터가 주어졌을 때 클래스 $k$에 속할 확률을 구하는 것이다. 베이즈 정리에 따르면 class posterior는 다음과 같다


$$
P(G=K\mid X=x) = \frac{f_k(x)\pi_k}{\sum_{l=1}^Kf_l(x)\pi_l}
$$



1. $\pi_k$ : 클래스 k의 사전확률로 $\sum_{l=1}^K\pi_l=1$이 되어야 한다
2. $f(x)$는 클래스 k에 속한 $X$의 확률분포다. Linear discriminant analysis 와 Quadratic discriminant analysis는 다변량 정규분포를 가정하고 있다.


$$
f_{k}(x)=\frac{1}{(2 \pi)^{p / 2}\left|\Sigma_{k}\right|^{1 / 2}} e^{-\frac{1}{2}\left(x-\mu_{k}\right)^{T} \Sigma_{k}^{-1}\left(x-\mu_{k}\right)}
$$



여기서 LDA는 가정사항을 하나 더 포함하고 있는데 클래스 별 covariance matrix가 동일하다고 가정한다.  따라서 클래스 $k$와 $l$의 log poserior odds는 다음과 같다


$$
\begin{array}{c}
\log \frac{\operatorname{Pr}(G=k \mid X=x)}{\operatorname{Pr}(G=\ell \mid X=x)}=\log \frac{f_{k}(x)}{f_{\ell}(x)}+\log \frac{\pi_{k}}{\pi_{\ell}} \\
=\log \frac{\pi_{k}}{\pi_{\ell}}-\frac{1}{2}\left(\mu_{k}+\mu_{\ell}\right)^{T} \Sigma^{-1}\left(\mu_{k}-\mu_{\ell}\right) +x^{T} \Sigma^{-1}\left(\mu_{k}-\mu_{\ell}\right)
\end{array}
$$


covariance matrix가 동일하다고 가정하고 있기 때문에 normalizing constant가 약분이 되고 exponent 안에 들어가는 quadratic part가 사라지게 된다. 따라서 클래스 $k$와 $l$을 분류하는 decision boundary $p(G=k\mid X=x)=p(G=l\mid X=x)$는 x에 linear한 형태다. 즉 $R^P$공간을 $K$개의 hyperplane으로 분리되는 것이다. 아래 그림은 클래스가 3개이고 $p=2$인 경우에 해당된다. 

![4](http://whdbfla6.github.io/assets/ml/4.3.PNG)

linear discriminant function은 클래스를 구분지어주는 함수로 $k$와 관련된 term만 남아 다음과 같이 구성된다. 최종적으로 linear discriminant function 값이 가장 크게 나오는 클래스 k로 분류가 된다.


$$
\delta_{k}(x)=x^{T} \mathbf{\Sigma}^{-1} \mu_{k}-\frac{1}{2} \mu_{k}^{T} \mathbf{\Sigma}^{-1} \mu_{k}+\log \pi_{k}
$$


여기서 정규분포의 파라미터들을 추정해야 하는데, 추정치는 다음과 같다.


$$
\begin{array}{l}
\hat{\pi}_{k}=N_{k} / N, \text { where } N_{k} \text { is the number of class- } k \text { observations }\\
\hat{\mu}_{k}=\sum_{g_{i}=k} x_{i} / N_{k}\\
\hat{\mathbf{\Sigma}}=\sum_{k=1}^{K} \sum_{q_{i}=k}\left(x_{i}-\hat{\mu}_{k}\right)\left(x_{i}-\hat{\mu}_{k}\right)^{T} /(N-K) .
\end{array}
$$


일반적으로 각 클래스에 대한 사전확률은 갖고 있는 데이터의 클래스 비율이다. 평균벡터는 MLE를 사용하고 있으며, 공분산행렬의 경우 pooled covariance를 사용한다. 

QDA의 경우 각 클래스 별로 Covariance matrix가 다르다고 가정한다. 따라서 위와같이 약분이 되지 않고, quadratic term이 남게 된다. quadratic discriminant function 형태는 다음과 같다.


$$
\delta_{k}(x)=-\frac{1}{2} \log \left|\Sigma_{k}\right|-\frac{1}{2}\left(x-\mu_{k}\right)^{T} \mathbf{\Sigma}_{k}^{-1}\left(x-\mu_{k}\right)+\log \pi_{k}
$$


$k$와 $l$의 decision boundary는 quadratic discriminant function이 같아지는 x들의 집합으로 아래와 같이 곡선 형태다. 

![4](http://whdbfla6.github.io/assets/ml/4.4.PNG)



### 3.3 Reduced Rank Linear Discriminant Analysis 



## 4. Logistic Regression

logistic regression은 decision boundary를 x에 linear한 function으로 모델링하고자 한다. 앞서 Y를 indicator function으로 구성해 linear function을 fitting하는 경우 각 추정치가 확률이 아닌 값을 가진다는 문제점이 있었다. 로지스틱 회귀의 경우 사후확률의 총합이 1이고 각 추정치들이 0과 1사이의 값을 가져 확률의 조건을 만족한다. 로지스틱 회귀는 log odds를 다음과 같이 구성한다.


$$
\begin{array}{c}
\log \frac{\operatorname{Pr}(G=1 \mid X=x)}{\operatorname{Pr}(G=K \mid X=x)}=\beta_{10}+\beta_{1}^{T} x \\
\log \frac{\operatorname{Pr}(G=2 \mid X=x)}{\operatorname{Pr}(G=K \mid X=x)}=\beta_{20}+\beta_{2}^{T} x \\
\vdots \\
\log \frac{\operatorname{Pr}(G=K-1 \mid X=x)}{\operatorname{Pr}(G=K \mid X=x)}=\beta_{(K-1) 0}+\beta_{K-1}^{T} x
\end{array}
$$


모델은 $K-1$개의 log odds로 구성되며, 일반적으로 마지막 클래스의 사후확률을 log odds의 분모로 사용한다. 위 식을 계산해 정리하면 각 클래스의 사후확률은 다음과 같이 나온다. 식의 형태를 보면 각 클래스에 속할 확률의 총 합이 1이고, 각 사후확률 값이 0과 1사이의 범위에 속하는 것을 확인할 수 있다.


$$
\begin{array}{l}
\operatorname{Pr}(G=k \mid X=x)=\frac{\exp \left(\beta_{k 0}+\beta_{k}^{T} x\right)}{1+\sum_{\ell=1}^{K-1} \exp \left(\beta_{\ell 0}+\beta_{\ell}^{T} x\right)}\quad k=1, \ldots, K-1, \\
\operatorname{Pr}(G=K \mid X=x)=\frac{1}{1+\sum_{\ell=1}^{K-1} \exp \left(\beta_{\ell 0}+\beta_{\ell}^{T} x\right)}
\end{array}
$$


### 4.1 Fitting Logistic Regression Models

일반적으로 로지스틱회귀의 베타값은 maximum likelihood estimator로 추정된다. $X$의 확률분포

### 4.5 Logistic Regression of LDA?



## 5. Separating Hyperplanes



### 5.1 Rosenblatt's Perceptron Learning Algorithm





