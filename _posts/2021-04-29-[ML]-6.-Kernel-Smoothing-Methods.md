---
title: '[ML] 6. Kernel Smoothing Methods'
use_math: true
comments: true
layout: single
classes: wide
categories:

  - 머신러닝
  - ESL

tags:
  
  - 머신러닝
  - ESL

---

##  Kernel Smoothing Methods

이 단원에서는 regression function $f(x)$ 를 추정하는데 있어서 유연성을 얻기 위한 테크닉을 다룬다. 유연성을 확보하기 위해 $x_0$ 근방의 데이터만 사용해서 fitting을 하게 되는데 이 때 kernel을 사용한다. $K_{\lambda}(x_0,x_i)$ kernel은 $x_0$로부터 $x_i$ 까지의 거리를 기반으로 해서 각 $x_i$ 에 가중치를 주며, 파라미터 $\lambda$는 neighborhood의 너비를 나타낸다. 

## 1. One dimensional Kernel Smoothers

>  K-Nearest neighborhoods 



$$
\hat{f}(x)=\operatorname{Ave}\left(y_{i} \mid x_{i} \in N_{k}(x)\right)
$$



K-Nearest neighborhoods  방법은 target point 근처의 k개의 점의 평균값을 구해서 함수를 추정한다. 



<p align = "center"><img src="http://whdbfla6.github.io/assets/ml/6.1.PNG" alt="4.1" style="zoom:80%;"  /> </p>



해당 그림은 $x_0$ 에 가까운 30개의 점의 평균값을 이용해 함수를 추정한 예시로 함수의 모양이 울퉁불퉁한 것을 확인할 수 있다. $x_0$가 왼쪽에서 오른쪽으로 이동할 때 더 가까운 점이 발견될 때까지 동일한 $x$값을 이용해 fitting을 하기 때문에 discontinuous 하며, 함수의 모양이 discrete하게 변화하는 것을 확인할 수 있다. 이는 모든 근처 점에 대해서 동일한 가중치를 주기 때문에 발생하는 문제이며, 거리에 따라 서로다른 가중치를 주어 불연속성 문제를 해결할 수 있다. 



>  Epanechnikov kernel



$$
\underset{\beta_{0}}{\operatorname{argmin}} \sum_{i=1}^{n}\left(Y_{i}-\beta_{0}\right)^{2} K_{h}\left(X_{i}-x\right)
$$



이 또한 constant를 fitting하는 방식이나, 가까운 거리에 있는 점에 더 높은 가중치를 부여한다.  이 식을 최소화하기 위해 $\beta_0$ 에 대해 미분을 하면 각 target point에서의 함수 추정치는 다음과 같은 형태다. 



$$
\hat{f}\left(x_{0}\right)=\frac{\sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right) y_{i}}{\sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right)}
$$



kernel 함수의 형태는 다음과 같다. 여기서 t의 절대값이 1보다 작은 경우는 $\mid x-x_{0}\mid < \lambda$ 인 경우로, 이는 $x_0$로부터 x까지의 거리가 $\lambda$보다 작은 경우에만 0이 아닌 값을 가지는 것을 의미한다. 따라서 $\lambda$값을 통해 neighborhood의 너비를 조정할 수 있다. 



$$
\begin{array}{c}
K_{\lambda}\left(x_{0}, x\right)=D\left(\frac{\left|x-x_{0}\right|}{\lambda}\right)\cdots (a) \\
D(t)=\left\{\begin{array}{ll}
\frac{3}{4}\left(1-t^{2}\right) & \text { if }|t| \leq 1 ; \\
0 & \text { otherwise }
\end{array}\right.
\end{array}
$$



<p align = "center"><img src="http://whdbfla6.github.io/assets/ml/6.2.PNG" alt="4.1" style="zoom:80%;"  /> </p>



추정된 함수를 보면 연속성이 보장되고 smooth해진 것을 확인할 수 있다. 



> Adaptive neighborhoods with kernel

$(a)$ 식에서와 다르게 $\lambda$ 값을 constant로 부여하는 것이 아니라 function형태로 나타낼 수도 있다. 



$$
K_{\lambda}\left(x_{0}, x\right)=D\left(\frac{\left|x-x_{0}\right|}{h_{\lambda}\left(x_{0}\right)}\right) .
$$



>  details 

- $\lambda$ 가 크다는 것은 $x_0$에서 멀리 떨어진 데이터도 고려하는 것이기 때문에 bias는 커지지만, variance는 낮아진다
- boundary issue: metric neighborhood의 방법의 경우 일정한 너비 안의 점만 사용하기 때문에 boundary에서 상대적으로 더 적은 point를 포함할 수 밖에 없다. knn기반은 boundary 근처에 갈수록 neighborhood의 너비가 넓어진다
- metric 기반은 근처의 데이터만 사용하기 때문에 bias가 일정하지만, knn 기반은 데이터가 부족한 경우 먼 위치의 데이터까지 사용해야 하기 때문에 bias가 훨씬 커지는 문제점이 있다
- metric 기반은 구간에 따라서 사용하는 데이터의 수가 달라 variance가 local density에 반비례한다 (local density가 높을수록 데이터 수가 많은 것을 의미하기 때문)  knn기반은 각 target point에 대해 사용하는 데이터의 수가 일정하기 때문에 variance가 constant하다.

### 1-1 Local Linear Regression

위에서 살펴본 Locally weighted average는 constant로 fitting을 하기 때문에 boundary에서 bias가 커진다는 문제가 있다. 



<p align = "center"><img src="http://whdbfla6.github.io/assets/ml/6.3.PNG" alt="4.1"/> </p>



왼쪽 boundary의 경우 오른쪽 영역의 데이터에만 가중치가 부여되기 때문에 커널이 비대칭하게 적용된다. 이 그림의 경우 왼쪽 boundary의 true function이 양의 기울기를 가져 boundary 부분에서 target point보다 큰 평균값을 갖는 것을 확인할 수 있다.

local linear regression은 constant로 fitting하였을 경우 boundary에서 bias가 커질 수 있는 문제점을 잡아준다. 결론부터 말하면 first order에 대한 bias correction을 해결할 수 있다. 각각의 추정치는 가중치가 커널로 부여된 weighted least square 문제를 풀어 얻을 수 있으며, 각 target point에서의 함수 추정치는 다음과 같다.



$$
\min _{\alpha\left(x_{0}\right), \beta\left(x_{0}\right)} \sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right)\left[y_{i}-\alpha\left(x_{0}\right)-\beta\left(x_{0}\right) x_{i}\right]^{2}
$$

$$
\hat{f}\left(x_{0}\right)=b\left(x_{0}\right)^{T}\left(\mathbf{B}^{T} \mathbf{W}\left(x_{0}\right) \mathbf{B}\right)^{-1} \mathbf{B}^{T} \mathbf{W}\left(x_{0}\right) \mathbf{y}
$$



- $b(x_0) = (1 x_0)$
- $B$: $b(x_i)^T$를 행으로 갖는 $N\times 2$ 행렬
- $W(x_0)$: $K_{\lambda}(x_0,x_i)$ 가중치를 대각성분으로 갖는 대각행렬



$\hat{f}(x_0)$은 $\sum_{i=1}^Nl_i(x_0)y_i$ 로도 나타낼 수 있으며 각  $y_i$에 linear한 형태임을 확인할 수 있다

이제 local linear regression이 자동으로 bias에 대한 first order를 보정해준다는 사실을 증명해볼 것이다. bias는 $E(\hat{f}(x_0))-f(x_0)$ 이며, bias를 구하기 앞서 $E(\hat{f}(x_0))$ 를 taylor expansion으로 나타내보자.



$$
\begin{aligned}
\mathrm{E} \hat{f}\left(x_{0}\right)=& \sum_{i=1}^{N} l_{i}\left(x_{0}\right) f\left(x_{i}\right) \\
=& f\left(x_{0}\right) \sum_{i=1}^{N} l_{i}\left(x_{0}\right)+f^{\prime}\left(x_{0}\right) \sum_{i=1}^{N}\left(x_{i}-x_{0}\right) l_{i}\left(x_{0}\right)
+\frac{f^{\prime \prime}\left(x_{0}\right)}{2} \sum_{i=1}^{N}\left(x_{i}-x_{0}\right)^{2} l_{i}\left(x_{0}\right)+R
\end{aligned}
$$



여기서 $\sum_{i=1}^{N}\left(x_{i}-x_{0}\right) l_{i}\left(x_{0}\right)=0, \quad \sum_{i=1}^{N} l_{i}\left(x_{0}\right)=1$ 이기 때문에 bias를 구하면 $\frac{f^{\prime \prime}\left(x_{0}\right)}{2} \sum_{i=1}^{N}\left(x_{i}-x_{0}\right)^{2} l_{i}\left(x_{0}\right)+R$  부분만 남게 된다. 즉 bias가 2차 미분 이상의 term에만 의존하고 있음을 확인할 수 있다. 



### 1-2 Local Polynomial Regression

이번에는 local linear fit에서만 그치지 말고 d차 다항식에 fitting을 해볼 것이다.



$$
\min _{\alpha\left(x_{0}\right), \beta_{j}\left(x_{0}\right), j=1, \ldots, d} \sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right)\left[y_{i}-\alpha\left(x_{0}\right)-\sum_{j=1}^{d} \beta_{j}\left(x_{0}\right) x_{i}^{j}\right]^{2}
$$



다음 문제를 풀어 target point $x_0$에 대한 함수 추정치를 구하면 $\hat{f} (x_{0}) = \hat{\alpha} (x_{0}) + \sum_{j=1}^d \hat{\beta_j} (x_{0}) x_0^j$ 이다. 



<p align = "center"><img src="http://whdbfla6.github.io/assets/ml/6.5.PNG" alt="4.1"/> </p>



이 그림을 살펴보면 local quadratic 을 사용했을 때 곡률에 대한 bias가 보정된 것을 확인할 수 있다.  하지만 tail 부분에서 bias가 줄어드는 것에 비해 variance가 커진다는 문제가 있다. 아래 그림은 다항식의 차수가 각각 $d=0,\ 1,\ 2$ 인 경우에 따른 variance curve로,  차수가 높아짐에 따라 꼬리 부분에서 variance가 급격히 커지는 것을 확인할 수 있다. 



<p align = "center"><img src="http://whdbfla6.github.io/assets/ml/6.6.PNG" alt="4.1"/> </p>



## 2. Selecting the Width of the Kernel



- Epanechnikov 나 tri-cube kernel에서 $\lambda$는 support region의 반지름을 나타낸다($\mid x-x_{0}\mid < \lambda$ 인 경우에 가중치가 부여되기 때문)
- Gaussian kernel에서 $\lambda$는 standard deviation $\Rightarrow \phi(x) = exp(-\frac{1}{2}x^2)$

$\lambda$값을 조정해 평균적인 window의 너비에 변화를 줄 수 있는데, 이에 따른 bias-variance tradeoff를 살펴보자

- window가 좁은 경우에 $x_0$에 가까운 $y_i$값만을 사용해 함수 추정치를 구하기 때문에 분산이 상대적으로 크지만, bias는 작아진다
- 이와 반대로 window가 넓어지면 상대적으로 먼 위치에 있는 점을 이용해서 함수를 추정하기 때문에 bias는 크지만 variance는 작아진다
- window가 무한대로 넓어지는 경우에 전체 데이터를 이용하기 때문에 global linear squares fit에 근접하게 된다. 



## 3. Local regression in $R^p$

이번에는 p차원의 x값을 이용해서 local regression을 구해보자. $b(X)$ 가 최대 차수가 d인 $X$에 대한 polynomial term을 원소로 갖는 벡터라고 하자. $d=1\ p=2$ 인 경우에 $b(X)=(1,X_1,X_2)$ 이며, $d=2\ p=2$ 인 경우에 $b(X) = (1,X_1,X_2,X_1^2,X_2^2,X_1X_2)$ 이다. 각 target point $x_0$에 대해 아래의 문제를 풀면 $\hat{f} (x_0) = b(x_0)^T\hat{B}(x_0)$ 을 얻을 수 있다.


$$
\min _{\beta\left(x_{0}\right)} \sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right)\left(y_{i}-b\left(x_{i}\right)^{T} \beta\left(x_{0}\right)\right)^{2}
$$


일반적으로 kernel은 벡터의 유클리디안 norm으로 계산된다. 여기서 특정 coordinate의 크기가 큰 경우 norm이 해당 coordinate에 의존하기 때문에 각 predictor를 표준화한 후에 kernel을 적용하는 것이 좋다. 


$$
K_{\lambda}\left(x_{0}, x\right)=D\left(\frac{\left\|x-x_{0}\right\|}{\lambda}\right)
$$


차원의 크기가 커지는 경우에 local regression을 사용하는 것이 어려워지는 문제점이 있다.  $p$의 크기가 2보다 커지는 경우에  interior가 아닌 boundary에 데이터의 양이 많아져 데이터의 불균형이 심해진다. 또한 neighborhood의 부피가 전체 공간의 부피와 가까워져 local regression을 하는 것의 의미가 사라지게 된다. 



## 4. Structured Local Regression Models in $R^p$



## 5. Local Likelihood and Other Models

일반적인 회귀모형에서 벗어나 target point 별로 kernel을 적용해 함수 추정치를 얻었듯이 다른 모수적인 방법도 local한 방법론을 적용할 수 있다. 그 예시를 살펴보자

> local likelihood

MLE 방법론을 통해 $\beta$의 추정값을 구하기 위해 log likelihood 를 구하면 다음과 같다. 


$$
l(\beta) = \sum_{i=1}^N l(y_i,x^T\beta) = \frac{1}{\left(2 \pi \sigma^{2}\right)^{n / 2}} \exp \left[\frac{-1}{2 \sigma^{2}} \sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\beta_{1} x_{i}\right)^{2}\right]
$$


하지만 전체 데이터가 아닌 각 $x_0$에 대해 local한 local likelihood를 구할 수도 있다. 이 경우에 커널을 적용해 일부 데이터만 사용해서 각 target point 별로 mle를 얻을 수 있다. 


$$
l\left(\beta\left(x_{0}\right)\right)=\sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right) l\left(y_{i}, x_{i}^{T} \beta\left(x_{0}\right)\right) .
$$


> multiclass linear logistic regression의 local 버전

global한 로지스틱 회귀에서 데이터가 주어졌을 경우 class j에 속할 확률은 다음과 같으며, 각 추정치는 mle다. 


$$
\operatorname{Pr}(G=j \mid X=x)=\frac{e^{\beta_{j 0}+\beta_{j}^{T} x}}{1+\sum_{k=1}^{J-1} e^{\beta_{k}+\beta_{k}^{T} x}}
$$


이 때 각 class $J$ 에 대한 local log-likelihood는 다음과 같다. class $J$에 속하면서 $x_0$의 근처 점만을 사용해서 log-likelihood를 구하는 것이다. 


$$
\begin{aligned}
\sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right) &\left\{\beta_{g_{1} 0}\left(x_{0}\right)+\beta_{g_{i}}\left(x_{0}\right)^{T}\left(x_{i}-x_{0}\right)\right.\\
&\left.-\log \left[1+\sum_{k=1}^{J-1} \exp \left(\beta_{k 0}\left(x_{0}\right)+\beta_{k}\left(x_{0}\right)^{T}\left(x_{i}-x_{0}\right)\right)\right]\right\} .
\end{aligned}
$$


$(x_i-x_0)$ 값을 넣어주는 것은 $x_0$로 centering하는 것을 의미하는데 이 경우 regression에서 기울기는 동일하고 상수 term에서만 차이가 발생한다. centering을 해주는 경우에 mle를 구한 뒤에 $\beta_{k0}$ 값만 비교해서 분류를 할 수 있다는 장점이 있다. 



## 6. Kernel Density Estimation and Classification



### 6-1 Kernel Density Estimation

random sample $x_1,x_2,\cdots,x_N$ 이 있다고 할 때 $x_0$ 에서의 probability density $\hat{f}(x_0)$를 유도해보자. natural local estimate은 다음 형태를 갖는다.


$$
\hat{f}_{X}\left(x_{0}\right)=\frac{\# x_{i} \in \mathcal{N}\left(x_{0}\right)}{N \lambda}
$$


분자에 있는 $\mathcal{N}\left(x_{0}\right)$ 은 $x_0$를 근처의 small metric neighborhood로  $\lambda$ 만큼의 너비를 갖는다. 이 추정치는 울퉁불퉁한 형태이기 때문에 아래의 smooth parazen estimate가 더 선호된다. 


$$
\hat{f}_{X}\left(x_{0}\right)=\frac{1}{N \lambda} \sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right)\cdots (b)
$$


관측치를 단순히 세는 것이 아니라 $x_0$에 가까운 관측치에 더 높은 가중치를 부여하는 방식이며, gaussian kernel이 가장 많이 사용된다. 평균이 0이고 standard deviation이 $\lambda$ 인 경우에 $(b)$ 의 form은 다음과 같이 나타낼 수 있다.


$$
\hat{f}_{X}(x)=\frac{1}{N} \sum_{i=1}^{N} \phi_{\lambda}\left(x-x_{i}\right)
$$


density의 추정치는 각 관측치 $x_i$가 평균이고 standard deviation이 $\lambda$인 normal distribution $N$개에 대한 평균이라고 볼 수 있다.  



<p align = "center"><img src="http://whdbfla6.github.io/assets/ml/6.7.png" alt="4.1"/> </p>



### 6-2 Kernel Density Classification

베이즈 정리를 이용해 $X$가 주어진 경우에 클래스 $j$ 에 속할 확률 추정치를 구하면 다음과 같다.


$$
\hat{\operatorname{Pr}}\left(G=j \mid X=x_{0}\right)=\frac{\hat{\pi}_{j} \hat{f}_{j}\left(x_{0}\right)}{\sum_{k=1}^{J} \hat{\pi}_{k} \hat{f}_{k}\left(x_{0}\right)}
$$


LDA에서는 $f_k(x)$를 gaussian distribution으로 가정하고 각 추정치로 mle를 사용했다. 비모수적인 방법론에서는 $\hat{f}_k(x)$ 를 추정하기 위해 앞서 살펴본 kernel density estimation을 사용한다. $\hat{\pi}_j$ 추정치는 LDA와 동일하게 sample proportion이다. 



### 6-3 The Naive Bayes Classifier

naive bayes classifier는 각 feature $X_k$가 독립이라고 가정하며, 차원이 높은 경우에 각 확률분포를 다음과 같이 쉽게 정의할 수 있다. 


$$
f_{j}(X)=\prod_{k=1}^{p} f_{j k}\left(X_{k}\right) \cdots (c)
$$


따라서 각 $\hat{f_{jk}} (X_{k})$ 는 one-dimensional kernel density 로 쉽게 추정할 수 있다. $X_j$ 가 discrete한 경우에는 히스토그램을 추정치로 사용한다. 

실제로는 각 feature들의 dependent하기 때문에 해당 가정사항을 충족하기 어렵다. 그럼에도 naive bayes classifier를 많이 사용하는 것은 density estimate이 biased함에도 불구하고 사후 확률에 대한 좋은 추정치를 만들어내기 때문이다.

$(c)$를 시작으로 logit transform을 구하면 다음과 같다



$$
\begin{aligned}
\log \frac{\operatorname{Pr}(G=\ell \mid X)}{\operatorname{Pr}(G=J \mid X)} &=\log \frac{\pi_{\ell} f_{\ell}(X)}{\pi_{J} f_{J}(X)} \\
&=\log \frac{\pi_{\ell} \prod_{k=1}^{p} f_{\ell k}\left(X_{k}\right)}{\pi_{J} \prod_{k=1}^{p} f_{J k}\left(X_{k}\right)} \\
&=\log \frac{\pi_{\ell}}{\pi_{J}}+\sum_{k=1}^{p} \log \frac{f_{\ell k}\left(X_{k}\right)}{f_{J k}\left(X_{k}\right)} \\
&=\alpha \ell+\sum_{k=1}^{p} g_{k k}\left(X_{k}\right) .
\end{aligned}
$$



## 7. Radial Basis Functions and Kernels 





그림 출처

[그림1](https://newsight.tistory.com/128)

