--- 
title: '[NLP]Lecture 12'
usemathjax: true
use_math: true
comments: true
layout: single
classes : wide
categories:
  - NLP

---

본 글은 스탠포드 [CS 224N](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/) [subwords embedding] 강의를 들으며 정리한 글 입니다.

## 4. character-level to build word-level

### 1) learning character-level representations for part-of-speech tagging : CNN 활용

![](https://i.imgur.com/L0ZFGO4.png)

이 모델은 character-level임베딩을 cnn필터에 적용해 word-embedding을 구축하였다. 

> input 

$w$ 단어가 M개의 character로 구성되어있다고 하자 

$${c_1,c_2,...,c_M}$$

우리는 이 각각의 character $c_m$을 character embedding $r_m^{chr}$로 구성할 수 있다. 

$$r_1^{chr},...,r_M^{chr}$$

$k^{chr}$를 윈도우 사이즈라고 하면 양쪽에 $(k^{chr}-1)/2$사이즈의 패딩을 넣어 input 값으로 들어가게 된다.

> cnn 필터 적용

output size 공식 n-h+1에 따르면,
$M(character 수)+ (k^{chr}-1)(패딩) - k^{chr} +1 = M$ 으로 output size가 character 수와 동일해진다. 이러한 output은 필터의 개수 $cl_u$만큼 출력된다.

> Max-pooling 

convolution 필터를 적용한 output에 maxpooling을 취하면 $cl_u$길이의 벡터를 구할 수 있을 것이다. 이 벡터가 바로 character-level에서 산출한 word embedding이라 볼 수 있다


### 2) character-based LSTM to build word representation

![](https://i.imgur.com/GOr0tt5.png)

각각의 character embedding을 양방향 lstm의 인풋값으로 넣으며, 마지막에 출력된 hidden state를 연결해 word-level의 임베딩을 만든다

### 3) character-aware neural language model

![](https://i.imgur.com/B3ciJlQ.png)

> 개요

- character level의 input값을 넣은 word-level의 언어모델을 제시
- character embedding  --> CNN --> Highway Network --> LSTM --> 예측하는 구조  

> Input & CNN

각 단어들을 character 단위로 분리를 해 embedding 벡터를 인풋으로 넣게 된다. 따라서 인풋 행렬의 크기는 (단어의 길이 x embedding size)다. 그리고 Input matrix에 크기가 2,3,4인 cnn 필터를 적용하게 된다.

filter size 2 x 3개 - 8x3 output  
가로 : 9(absurdity 글자 수)-2(filter 크기)+1 = 8
세로 : 필터의 개수 3
filter size 3 x 4개 - 7x4 output
filter size 4 x 5개 - 6x5 output

> MAX pooling 

cnn필터를 적용한 output에 max-pooling을 적용하게 output의 길이는 항상 12로 고정될 것이다. 

> highway network 

![](https://i.imgur.com/6YlOL8V.png)

highway network는 deep neural network를 잘 학습시키기 위해 제안된 방법이다. Gating function을 이용해 인풋 신호를 **변환시키거나(transformation) 통과시키거나(bypass)** 해서 네트워크가 더욱 깊어질 수 있도록 하는 기법이다. T는 **Transformation gate**, (1-T)를 **Carry gate**라 하는데,  T=0인 경우에는 인풋값에 변형없이 아웃풋 값이 그대로 나오고 T=1인 경우에는 $W_h,b_h$에 의해 변형된 값이 비선형함수 $g$를 지나 아웃풋으로 나오게 된다. highway network를 통과한 결과물은 word-level이 되고 LSTM network의 input값으로 들어가게 된다. 

> Results

![](https://i.imgur.com/vjj0ho5.png)

이 논문의 경우 perplexity를 기준으로 모델의 성능을 평가하고 있는데, perplexity는 다음과 같이 계산할 수 있다. 

$$PPL = exp(NLL/T)$$

T는 예측시에 사용되는 Sequence의 길이, NLL은 Negative log likelihood로 Test 데이터를 대상으로 계산되는 loss값이다. 따라서 PPL값이 낮을수록 좋은 성능을 보이는 것으로 해석할 수 있다. 위에 표를 보면 Charcter level을 인풋으로 사용했을 때 *적은 파라미터로 더 좋은 성능*을 보이는 것을 확인할 수 있다

![](https://i.imgur.com/KFV3hTz.png)

- word: 사람이름이 순서대로 나옴
- char(before highway): 예측을 잘 못함
- char(after highway): highway를 거친 뒤에 사람이름이 나옴

![](https://i.imgur.com/1XVKwAv.png)

- word: oov 문제를 해결할 수 없으며, 주로 원본에서 copy해 옴
- char(before highway): 처리 가능하나 결과가 좋지는 않음
- char(after highway): 가장 좋은 결과


### 4) hybrid NMT

> 개요 

- word-level 모델과 character-level 모델의 절충안
- Word-level의 모델들은 단어장에 없는 단어(oov)에 대해서 <unk>를 반환하고, source language에서 상응하는 단어를 그대로 가져오는 방식이다. 
- character level의 경우 학습속도가 느려서 현실적으로 사용하기 어려움
- hybrid NMT는 word-level을 기본으로 하여 없는 단어에 대해서만 character-level을 사용하는 방식


> source sentence의 character-level 표현

![](https://i.imgur.com/LOPy7pT.png)

이 그림에서는 cute이라는 단어가 희귀한 단어이기 때문에 <unk>에 해당한다. 따라서 'c','u','t','e','_' character 단위로 쪼개 lstm의 인풋값으로 넣고 마지막 Hidden state를 cute 단어에 대한 임베딩 벡터로 사용한다.

> target sentence의 character-level 표현

일반적인 word-base NMT는 모르는 단어에 대해서 <UNK> 토큰을 반환한다. 이 때 Hybrid NMT는 모르든 단어에 대해서 Character-level로 다시 수행하게 된다. 따라서 loss는 아래와 같이 구성된다.

$$J = J_W + \alpha J_c$$

- $J_W$ : "un","<unk>","chat","_"에 대한 negative log likelihood
- $J_c$ : character level decoder에서 문자를 예측할 때 발생하는 loss 

## 5. FastText embedding

FastText는 페이스북에 개발해 공개한 단어 임베딩 방법으로, 각 단어를 character단위 n-gram으로 표현한다. 기존 word2vec에서 발전된 형태라고 볼 수 있다. 

FastText 모델에서는 *where*이라는 단어를 6개의 **문자 단위의 n-gram 벡터 합**으로 표현된다.(n=3)

$$U_{where} = Z_{<wh} + Z_{whe} + Z_{her} + Z_{ere} + Z_{re>} + Z_{<where>}$$

Word2Vec에서 $\frac{1}{1+exp(-u_tv_c)}$를 최대화하는 방식이었다면($U$는 타깃단어, $V$는 문맥단어), FastText에서는 $\frac{1}{1+exp(-\sum{Z_gv_c})}$을 최대화하는 방식으로 훈련이 진행된다. 이는 벡터 $Z$와 $v_c$의 내적 값을 높이는 것과 동일하다. 따라서 타깃 단어 Where을 구성하는 n-gram 벡터와 문맥 단어의 벡터 간 유사도를 높여야 한다는 의미로 이해할 수 있다.


## 참고 문헌

1. [hybrid NMT](https://towardsdatascience.com/a-hybrid-neural-machine-translation-model-luong-manning-fcf419be358a)
2. [Stanford CS 224N 강의 Lecture12](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/)
3. 한국어임베딩
